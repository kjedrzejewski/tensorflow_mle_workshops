{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Height and weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains 101 observations of people's weights and heights. The relationship between weight and height can be expressed by equation $weight = a \\cdot height ^ b$, where $a$ and $b$ are parameters. The task is to estimate these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tensorflow)\n",
    "library(plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_height <- readr::read_csv('data/weight_height.csv') # read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the data\n",
    "weight_height %>% \n",
    "  plot_ly() %>% \n",
    "  add_markers(x = ~h, y = ~w) %>%\n",
    "  layout(xaxis = list(title = 'Height'), \n",
    "         yaxis = list(title = 'Weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# express data as nodes\n",
    "ta <- NA # TODO: create a variable for a. Initialise it with 1.0.\n",
    "tb <- NA # TODO: create a variable for b. Initialise it with 1.0.\n",
    "\n",
    "th <- NA # TODO: create a constant for height. \n",
    "tw <- NA # TODO: create a constant for weight. \n",
    "\n",
    "w_ <- NA # TODO: calculate weight according to the equation in instruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model \n",
    "loss <- NA # TODO: from losses choose MSE as loss function, \n",
    "           # set appropriate: labels (observed data) and predictions (estimations)\n",
    "optimizer <- NA # TODO: from train choose Adam Optimizer and set learning rate= 0.05\n",
    "\n",
    "train <- NA #TODO: minimize loss in a given optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf$Session() # run session \n",
    "sess$run(tf$global_variables_initializer()) # init variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 10000 # number of iterations to run\n",
    "\n",
    "# table for storing model parameter optimisation progress log\n",
    "r = tibble(\n",
    "  it = 1:iter,\n",
    "  loss = rep(NA_real_, iter),\n",
    "  a = rep(NA_real_, iter),\n",
    "  b = rep(NA_real_, iter)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for(i in 1:iter){\n",
    "  NA # TODO: run one iteration of training \n",
    "  r$loss[i] = sess$run(loss)\n",
    "  r$a[i] = sess$run(ta)\n",
    "  r$b[i] = sess$run(tb)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly(data = r) %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~loss,\n",
    "    name = 'loss'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly(data = r) %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~a,\n",
    "    name = 'a'\n",
    "  ) %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~b,\n",
    "    name = 'b'\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <b>Likelihood</b> measures the plausibility of a model parameter value, given observations we have.<br/>\n",
    "    $\\mathcal{L}(\\theta \\mid x) = p_\\theta (x) = P (X=x; \\theta)$, where\n",
    "    <ul>\n",
    "        <li>$\\mathcal{L}(\\theta \\mid x)$ is the likelihood value</li>\n",
    "        <li>$p_\\theta (x) = P (X=x; \\theta)$ is the probability of observing $x$ given parameter values are equal $\\theta$</li>\n",
    "        <li>$x$ observed data</li>\n",
    "        <li>$\\theta$ parameter values</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "    In other words, it tells us what is the probability of observing what we observed, assuming the given model parameter values. The value of this probability changes depending on the $\\theta$. When these parameters have values that do not let $x$ occur, $P (X=x; \\theta) = 0$, and when it has values in which $x$ must always occur $P (X=x; \\theta) = 1$.\n",
    "</p>\n",
    "<p>\n",
    "    Of course, we usually have more than one observation: $x_1$, $x_2$, $x_3$, ... So its likelihood is equal to:<br/>\n",
    "    $\\mathcal{L}(\\theta \\mid x_1, x_2, ..., x_n) = P (X=[x_1, x_2, ..., x_n]; \\theta) = \\prod_{i=1}^n P (X=x_i; \\theta)$<br/>\n",
    "</p>\n",
    "<p>\n",
    "    The $\\theta$ for which the value of $\\prod_{i=1}^n P (X=x_i; \\theta)$ is highest is called the Maximum Likelihood Estimate, and in the frequentist statistics it is assumed to be an accurate estimation of a latent parameter (a parameter that can't be measured directly).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Likelihood exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You tossed a coin three times. You observed heads once and tails two times. What are the likelihoods that the probability of the coin landing heads up is equal to:\n",
    "<ul>\n",
    "    <li>$1 \\over 2$</li>\n",
    "    <li>$1 \\over 3$</li>\n",
    "    <li>$1 \\over 5$</li>\n",
    "    <li>$11 \\over 30$</li>\n",
    "    <li>$9 \\over 30$</li>\n",
    "</ul>\n",
    "What do you think, which of these probabilities is the most likely assuming you don't know what probability you should expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Cross-entropy and MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Now, let's take the equation from earlier<br/>\n",
    "    $\\mathcal{L}(\\theta \\mid x_1, x_2, ..., x_n) = \\prod_{i=1}^n P (X=x_i; \\theta)$\n",
    "</p>\n",
    "<p>\n",
    "    and logarithmise both sides of this equation:<br/>\n",
    "    $log \\mathcal{L}(\\theta \\mid x_1, x_2, ..., x_n) = log\\prod_{i=1}^n P (X=x_i; \\theta) = \\sum_{i=1}^n log P (X=x_i; \\theta)$<br/>\n",
    "    logarithm of the likelihood is called the log-likelihood and has the same maximum as the likelihood because logarithm is a monotonically increasing function.\n",
    "</p>\n",
    "<p>\n",
    "    Now, let's assume that we have a probability that something happened ($y_i = 1$), e.g. a student answered a test question correctly $P (Y=1; \\theta)$, then the probability of that not occuring ($y_i = 0$) in the same conditions - $P (Y=0; \\theta)$ - is equal to: $1 - P (Y=1; \\theta)$. Then:<br/>\n",
    "    $log \\mathcal{L}(\\theta \\mid y_i) = log P (Y=y_i; \\theta) = y_i \\cdot log P(Y=1; \\theta) + (1 - y_i) \\cdot log (1 - P(Y=1; \\theta))$\n",
    "</p>\n",
    "<p>\n",
    "    For simplification, let's denote $P(Y=1; \\theta)$ for given $y_i$ as $p_{i,\\theta}$. Now:<br/>\n",
    "    $log \\mathcal{L}(\\theta \\mid y_i) = y_i \\cdot log p_{i,\\theta} + (1 - y_i) \\cdot log (1 - p_{i,\\theta})$\n",
    "</p>\n",
    "<p>\n",
    "    And now let's multiply both sides by $-1$:<br/>\n",
    "    $-log \\mathcal{L}(\\theta \\mid y_i) = -(y_i \\cdot log p_{i,\\theta} + (1 - y_i) \\cdot log (1 - p_{i,\\theta})) = cross$-$entropy(p_{i,\\theta}, y_i)$\n",
    "</p>\n",
    "<p>\n",
    "    Thus:\n",
    "    <ul>\n",
    "        <li><b>when we minimise the value of cross-entropy we also maximise the likelihood</b> (because we minimise the negative log-likelihood)</li>\n",
    "        <li>what we do in TensorFlow is not magic that just works, it's a well established statistical method that has been in use for over a century</li>\n",
    "        <li>it also applies to very complicated models called neural networks. Just in this case, we do not believe that parameters we estimate make any sense</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Example - an unbalanced coin and MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages we will need\n",
    "library(tensorflow)\n",
    "library(tidyverse)\n",
    "library(plotly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will use TF and MLE to estimate the probability that an unbalanced coin lands heads up - $p_H$. Below we have a result of 100 coin tosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - HEADS, 0 - TAILS\n",
    "results = c(0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p = tf$Variable(0.01) # the variable containing a probability of a coin landing heads up. Let's initialise\n",
    "                        # it with 0.01 (assume this coin almost always lands tails up)\n",
    "t_y = tf$constant(results)\n",
    "\n",
    "p_vector = tf$fill(list(tf$size(t_y)), t_p) # we need to calculate the expected probability for each coin toss we observed\n",
    "                                            # however, it's the same in all attempts, so we just assume the current t_p estimate for each observation\n",
    "\n",
    "t_loss = tf$losses$log_loss(t_y, p_vector) # now we need to calculate the cross-entropy between estimated probability, and each observation\n",
    "\n",
    "# and now, we also need to establish the optimisation process...\n",
    "optimizer = tf$train$AdamOptimizer(0.1)\n",
    "train = optimizer$minimize(t_loss)\n",
    "\n",
    "# as always we need to create a session\n",
    "sess = tf$Session() \n",
    "sess$run(tf$global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 1000 # number of iterations to run\n",
    "\n",
    "# table for storing model parameter optimisation progress log\n",
    "r = tibble(\n",
    "    it = 1:iter,\n",
    "    loss = rep(NA_real_, iter),\n",
    "    prob = rep(NA_real_, iter)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we've got everything, so LET'S START THE ESTIMATION\n",
    "for(i in 1:iter){\n",
    "    sess$run(train)\n",
    "    r$loss[i] = sess$run(t_loss)\n",
    "    r$prob[i] = sess$run(t_p)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat('Final probability that the coin lands heads up is ', sess$run(t_p), '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we finished the estimation process, we can check how the loss value and the estimated probability was changing during the iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly() %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~loss,\n",
    "    data = r\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly() %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~prob,\n",
    "    data = r\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Example - MLE of normal distribution parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    In this section, we will do the maximum likelihood estimation of parameters for a continuous probability distribution. In such cases we need to use an alternative likelihood definition:<br/>\n",
    "    $\\mathcal{L}(\\theta \\mid x) = f (X=x; \\theta)$<br/>\n",
    "    where $f (X=x; \\theta)$ is the value of the probability density function (<i>pdf</i>)\n",
    "</p>\n",
    "<p>\n",
    "    Since we are working with the normal distribution, which have two parameters (mean and standard deviation), the total likelihood of all observations takes form:\n",
    "    $\\mathcal{L}(mean, sd \\mid x_1, x_2, ..., x_n) = \\prod_{i=1}^n f (X=x_i; mean, sd)$,\n",
    "    and<br/>\n",
    "    $log\\mathcal{L}(mean, sd \\mid x_1, x_2, ..., x_n) = \\sum_{i=1}^n log f (X=x_i; mean, sd)$\n",
    "</p>\n",
    "<p>\n",
    "    <b>QUESTION</b>: is there any reason why we should prefer one of these forms over another? (e.g. a sum of logarithms over a product)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages we will need\n",
    "library(tensorflow)\n",
    "library(tidyverse)\n",
    "library(plotly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with generating some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd_mean = 3   # the mean of our distribution\n",
    "nd_sd = 2     # the standard deviation of our distribution\n",
    "nd_data = rnorm(10000, nd_mean, nd_sd) # and the actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to build a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mean = tf$Variable(0.0) # a variable for the mean of the distribution, let's set it initially to 0\n",
    "t_sd = tf$Variable(1.0)   # a variable for the sd of the distribution, let's set it initially to 1\n",
    "\n",
    "dist = tf$distributions$Normal(loc = t_mean, scale = t_sd) # a distribution we will use, it takes variables storing mean and sd as parameters\n",
    "\n",
    "lprobs = dist$log_prob(nd_data) # log-likelihoods of parameters for each observation, i.e. logarithms of pdf values assuming current t_mean and t_sd \n",
    "negLL = -tf$reduce_sum(lprobs)  # sum of observations likelihoods. It's negative, because it is a value we will be minimising \n",
    "\n",
    "# of course, we also need to establish the optimisation process...\n",
    "optimizer = tf$train$AdamOptimizer(0.1)\n",
    "train = optimizer$minimize(negLL)\n",
    "\n",
    "# ... and create a session\n",
    "sess = tf$Session() \n",
    "sess$run(tf$global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 1000 # number of iterations to run\n",
    "\n",
    "# table for storing model parameter optimisation progress log\n",
    "r = tibble(\n",
    "    it = 1:iter,\n",
    "    negLL = rep(NA_real_, iter),\n",
    "    mean = rep(NA_real_, iter),\n",
    "    sd = rep(NA_real_, iter)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we've got everything, so let's start\n",
    "for(i in 1:iter){\n",
    "    sess$run(train)\n",
    "    r$negLL[i] = sess$run(negLL)\n",
    "    r$mean[i] = sess$run(t_mean)\n",
    "    r$sd[i] = sess$run(t_sd)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when our model parameters were fitted to the data, we can check how it went. Let's start with the likelihood maximisation (or rather minimisation of the negative log-likelihood):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly() %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~negLL,\n",
    "    data = r\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check how values of mean and standard deviation were changing during iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly(data = r) %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~mean,\n",
    "    name = 'mean'\n",
    "  ) %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~sd,\n",
    "    name = 'sd'\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Probability estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    OK, so now we know how we can measure how accurate is our probability estimation. Now, we only need to have a way to estimate the probability, i.e. a value that isn't lower than 0, and not higher than 1. We can use the log-odds (logit) and the logistic function for that purpose.\n",
    "</p>\n",
    "<p>\n",
    "    $\\operatorname{log-odds}(p)=\\operatorname{logit}(p)=\\log\\left( \\frac{p}{1-p} \\right)$<br/>\n",
    "    It converts a value from the interval $[0;1]$ to the interval $[-∞;+∞]$. And now, it's something that we can easily estimate using e.g. linear regression. Later, we just need to convert it back to a probability using a sigmoid function:<br/>\n",
    "</p>\n",
    "<p>\n",
    "    $p = \\operatorname{sigmoid}(logit) = \\frac{1}{1 + e^{-logit}}$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Example - linear logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    In this example we will use MLE to estimate parameters $a$, $b$ and $c$ of the logistic linear regression model. <br/>\n",
    "</p>\n",
    "<p>\n",
    "    $\\operatorname{logit}(z = 1) = a \\cdot x + b \\cdot y + c $\n",
    "</p>\n",
    "<p>\n",
    "    $\\operatorname{P}(z = 1) = \\frac{1}{1 + e^{-\\operatorname{logit}(z = 1)}}$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_351 = read_csv('data/example_351.csv')\n",
    "head(example_351)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data into the graph\n",
    "t_x = tf$constant(example_351$x, dtype = 'float32')\n",
    "t_y = tf$constant(example_351$y, dtype = 'float32')\n",
    "t_z = tf$constant(example_351$z, dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of variables storing parameter values\n",
    "t_a = tf$Variable(42) # as we know, this is a really good number\n",
    "t_b = tf$Variable(42)\n",
    "t_c = tf$Variable(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_logit = t_a * t_x + t_b * t_y + t_c  # log-odds of P(z = 1)\n",
    "t_z_ = tf$sigmoid(t_logit) # and the actual probability of z = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to create an optimiser, calculate loss, and minimise its value\n",
    "optimizer = tf$train$AdamOptimizer(learning_rate = 0.1)\n",
    "loss = tf$losses$log_loss(t_z, t_z_)\n",
    "train = optimizer$minimize(loss)\n",
    "\n",
    "sess = tf$Session()\n",
    "sess$run(tf$global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to train model parameters\n",
    "for(i in 0:2000){\n",
    "    sess$run(train)\n",
    "    if(i %% 200 == 0){\n",
    "        cat(i, ') ', format(Sys.time(), \"%Y-%m-%d %X\"), ' Loss: ', sess$run(loss), '\\n', sep = '')\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we can check what are the final parameter values\n",
    "cat(\"t_a: \", sess$run(t_a), \"\\n\")\n",
    "cat(\"t_b: \", sess$run(t_b), \"\\n\")\n",
    "cat(\"t_c: \", sess$run(t_c), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Exercise - A bribe-taking mayor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The mayor of Los Data-flowos is a well-known bribe-taker. The local data science club decided to find out in what conditions he is eager to take a bribe. They discovered two main factors that influence it.\n",
    "</p>\n",
    "<p>\n",
    "    The first one is the amount of money offered, which have a logarhitmic influence on the mayor, which is proportional to:<br/>\n",
    "    $mc \\cdot log(ao)$, where<br/>\n",
    "    <ul>\n",
    "        <li><b>mc</b> - money coefficient, currently unknown and needs to be estimated</li>\n",
    "        <li><b>ao</b> - amount of money offered by a briber</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "    The second one is the number of policemen potential briber knows, which have influence on mayor's decision proportional to:<br/>\n",
    "    $pc \\cdot np^{pp}$, where<br/>\n",
    "    <ul>\n",
    "        <li><b>pc</b> - coefficient for the number of known policemen, currently unknown and needs to be estimated</li>\n",
    "        <li><b>np</b> - number of policemen known by the briber</li>\n",
    "        <li><b>pp</b> - exponent for the number of policemen, currently unknown and needs to be estimated</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "    Members of the club were also able to collect some observations. Each consists of this observations consists of three features:\n",
    "    <ul>\n",
    "        <li><b>money</b> - amount of money offered</li>\n",
    "        <li><b>policemen</b> - number of policemen potential briber knows</li>\n",
    "        <li><b>bribe_accepted</b> - binary flag informing if mayor accepted the bribe</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "    Your task is to use TensorFlow to estimate parameter values of a model predicting if the mayor will take a bribe:\n",
    "    <ul>\n",
    "        <li><b>mc</b> called <b>v_money_coef</b> below</li>\n",
    "        <li><b>pc</b> called <b>v_policemen_coef</b> below</li>\n",
    "        <li><b>pp</b> called <b>v_policemen_pow</b> below</li>\n",
    "        <li><b>intercept</b> called <b>v_intercept</b> below</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bribers = read_csv('data/bribers.csv') %>%\n",
    "    mutate(\n",
    "        policemen = as.double(policemen)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a peek\n",
    "head(bribers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with loading the data into the graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_money = NA           # TODO: create a constant with amounts of money offered\n",
    "t_policemen = NA       # TODO: create a constant with numbers of policemen known\n",
    "t_bribe_accepted = NA  # TODO: create a constant with flags whether bribes was accepted in given attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and declaring variables for parameters you want to estimate. Set the initial value of each variable to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_money_coef = NA        # TODO: create a variable for money_coef (mc). Initialise it with 1.0.\n",
    "\n",
    "v_policemen_pow = NA     # TODO: create a variable for policemen_power (pp). Initialise it with 1.0.\n",
    "v_policemen_coef = NA    # TODO: create a variable for policemen_coef (pc). Initialise it with 1.0.\n",
    "\n",
    "v_intercept = NA         # TODO: create a variable for intercept. Initialise it with 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to build a graph describing, how probability of taking a bribe can be calculated using constants and variables. You also need to define a total loss, and the optimisation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money_comp = NA     # TODO: calculate money component as money_coef * log(money)\n",
    "policemen_comp = NA # TODO: calculate policemen component as policemen_coef * (policemen ^ policemen_power)\n",
    "\n",
    "logit = NA          # TODO: calculate logit as a sum of money component, policemen component and intercept\n",
    "prob = NA           # TODO: convert logit to a probability using a sigmoid function\n",
    "\n",
    "loss = NA           # TODO: calculate mean cross-entropy loss of the estimated probability\n",
    "\n",
    "optimizer = NA      # TODO: create an Adam optimiser with a learning rate equal 0.05\n",
    "train = NA          # TODO: use optimiser to minimise loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, it's time to create a session, and run the optimisation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf$Session()\n",
    "sess$run(tf$global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in 0:1000){\n",
    "    sess$run(train)\n",
    "    if(i %% 100 == 0){\n",
    "        cat(i, ') ', format(Sys.time(), \"%Y-%m-%d %X\"), ' Loss: ', sess$run(loss), '\\n', sep = '')\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When optimisation is finished, we can read parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(\"v_money_coef: \", sess$run(v_money_coef), \"\\n\")\n",
    "cat(\"v_policemen_pow: \", sess$run(v_policemen_pow), \"\\n\")\n",
    "cat(\"v_policemen_coef: \", sess$run(v_policemen_coef), \"\\n\")\n",
    "cat(\"v_intercept: \", sess$run(v_intercept), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Local fortune-teller is sure that these parameters should be equal:<br/>\n",
    "    <ul>\n",
    "        <li>money_coef = 1.2</li>\n",
    "        <li>policemen_pow = 1.3</li>\n",
    "        <li>policemen_coef = -0.5</li>\n",
    "        <li>intercept = -3.0</li>\n",
    "    </ul>\n",
    "    It seems these numbers are correct, as they often worked. Are your estimates similar?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How to check if your model works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now let's see how you can check if your model actually works. Generally, the easiest way to do that is to:\n",
    "    <ol>\n",
    "        <li>assume some parameters</li>\n",
    "        <li>generate synthetic data in a way expected by your model using these parameters</li>\n",
    "        <li>use the model you build to estimate parameters on these artificial data</li>\n",
    "        <li>compare parameters you originally assumed with these that your model estimated</li>\n",
    "        <li>improve the model to address issues you observed</li>\n",
    "    </ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 1PL IRT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    In this section we will work with the 1-parameter logistic item response theory model (1PL IRT). It is a psychometric model that assumes the probability of giving a correct answer by a student is equal:\n",
    "</p>\n",
    "<p>\n",
    "    $p(correct_{si} = 1) = {1 \\over {1+e^{-logit(correct_{si})}}}$<br/>\n",
    "    $logit(correct_{si} = 1) = skill_s - diff_i$, where\n",
    "    <ul>\n",
    "        <li>$p(correct_{si} = 1)$ is a probability that student <i>s</i> answers item <i>i</i> correctly</li>\n",
    "        <li>$skill_s$ is a skill level of a student <i>s</i></li>\n",
    "        <li>$diff_i$ is a difficulty level of an item <i>i</i></li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "    It means that:\n",
    "    <ul>\n",
    "        <li>When student's skill is equal to an item's difficulty the probability of a correct answer is at 50%</li>\n",
    "        <li>When student's skill is higher than an item's difficulty the probability of a correct answer is higher than 50%. E.g. when this difference is ~3, this probability is at ~95%</li>\n",
    "        <li>When student's skill is lower than an item's difficulty the probability of a correct answer is lower than 50%. E.g. when this difference is ~-3, this probability is at ~5%</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, we will draw values of parameters we will use later:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_count = 100\n",
    "student_count = 1000\n",
    "\n",
    "items = tibble(\n",
    "    item = factor(sprintf('item%05d',1:item_count)),\n",
    "    diff = rnorm(item_count, 0, 1)\n",
    ")\n",
    "\n",
    "students = tibble(\n",
    "    student = factor(sprintf('student%05d', 1:student_count)),\n",
    "    skill = rnorm(student_count, 0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's take a peek</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(items)\n",
    "head(students)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will generate synthetic data doing the simulation based on the 1PL IRT model assumptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = students %>%\n",
    "    mutate(tmp = 1) %>%               # tmp column is to enable us to do a cross join with items\n",
    "    inner_join(mutate(items, tmp = 1), by = 'tmp') %>%\n",
    "    select(-tmp) %>%\n",
    "    mutate(\n",
    "        logit = skill - diff,         # we calculate the logit values...\n",
    "        prob = 1/(1 + exp(-logit)),   # ... convert it to a probabilities ...\n",
    "        correct = prob >= runif(n())  # ... and now we draw correct and wrong answers using these probabilities\n",
    "    )\n",
    "  \n",
    "test_data = full_data %>%             # test_data has only information we would have in the production setting\n",
    "    select(student, item, correct) %>%\n",
    "    mutate(\n",
    "        student_ind = as.integer(student),\n",
    "        item_ind = as.integer(item)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data %>% # let's take a peek how these data look like\n",
    "    sample_n(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Use the model for parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Before we will use the model, we need to create it:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function building a computation graph up to the total loss\n",
    "build_1pl_irt_model <- function(irt_data){\n",
    "    # a vector for storing items' difficulties\n",
    "    t_diffs = tf$Variable(\n",
    "        tf$random_normal(\n",
    "           shape = shape(length(levels(irt_data$item))),\n",
    "           mean = 0,\n",
    "           stddev = 6.0\n",
    "        ),\n",
    "        name = 'diffs'\n",
    "    )\n",
    "  \n",
    "    # a vector for storing students' skill levels\n",
    "    t_skills = tf$Variable(\n",
    "        tf$random_normal(\n",
    "            shape = shape(length(levels(irt_data$student))),\n",
    "            mean = 0,\n",
    "            stddev = 6.0\n",
    "        ),\n",
    "        name = 'skills'\n",
    "    )\n",
    "  \n",
    "    # vectors for storing difficulties and skill levels related for given observation\n",
    "    # irt_data$item_ind and irt_data$student_ind are implicitly loaded as constants into the computation graph\n",
    "    t_diffs_gathered = tf$gather(t_diffs, irt_data$item_ind - 1L)      # we substract 1, because in python indexes start from 0 \n",
    "    t_skills_gathered = tf$gather(t_skills, irt_data$student_ind - 1L)\n",
    "\n",
    "    # now we calculate logits and probabilities for each observed data point\n",
    "    t_logit = t_skills_gathered - t_diffs_gathered\n",
    "    prob = tf$sigmoid(t_logit)\n",
    "\n",
    "    # a constant for the observation of a correct or a wrong answer\n",
    "    y = tf$constant(as.double(irt_data$correct))\n",
    "\n",
    "    # loss observed in a given row - cross-entropy\n",
    "    loss_each = tf$losses$log_loss(y, prob)\n",
    "    # loss averaged over all rows\n",
    "    loss = tf$reduce_mean(loss_each, name = 'loss')\n",
    "  \n",
    "    # we return some of the graph nodes, e.g. vectors with skill levels and difficulties\n",
    "    list(\n",
    "        t_diffs = t_diffs,\n",
    "        t_skills = t_skills,\n",
    "        t_diffs_gathered = t_diffs_gathered,\n",
    "        t_skills_gathered = t_skills_gathered,\n",
    "        prob = prob,\n",
    "        loss = loss\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that adds an optimiser and a training node to the graph\n",
    "add_training = function(m, sess, learning_rate = 0.1){\n",
    "  optimizer = tf$train$AdamOptimizer(learning_rate)\n",
    "  train = optimizer$minimize(m$loss)\n",
    "  \n",
    "  sess$run(tf$global_variables_initializer())\n",
    "  \n",
    "  m$optimizer = optimizer\n",
    "  m$train = train\n",
    "  \n",
    "  m\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now when we have functions building a model, we need to call them and build the actual model instance that uses the data we have. Please keep in mind, that because observations are kept in the graph as constants, it needs to be built separately for each dataset we want fit it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_1pl_irt_model(test_data)\n",
    "sess = tf$Session()\n",
    "model = add_training(model, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do a single step of forward and backward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(model$train)       # each time we run this node, one phase of forward and backward propagation is performed\n",
    "print(sess$run(model$loss)) # now we can check the value of the loss in the given iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's do a thousand of steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in 0:1000){\n",
    "    sess$run(model$train)\n",
    "    if(i %% 100 == 0){\n",
    "        cat(i, ') ', format(Sys.time(), \"%Y-%m-%d %X\"), ' Loss: ', sess$run(model$loss), '\\n', sep = '')\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we fitted model to the data, we can extract the parameters value from the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's extract the estimated difficulty and add it to the items tibble...\n",
    "items_with_est = cbind(items, est_diff = sess$run(model$t_diffs))\n",
    "# ... and the estimated skill to the students tibble\n",
    "students_with_est = cbind(students, est_skill = sess$run(model$t_skills))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Check if these model parameters are estimated correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start, we can check if estimated values correlated with original ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we check if these estimates correlate with \n",
    "items_with_est %>%\n",
    "    summarise(cor(diff, est_diff))\n",
    "\n",
    "# and check the correlation of original and estimated values\n",
    "students_with_est %>%\n",
    "    summarise(cor(skill, est_skill))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems, they correlate quite well. Let's check how accurately each of skill levels and difficulties are estimated. We can do that using a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(plotly )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly() %>%\n",
    "  add_markers(\n",
    "    x = ~diff,\n",
    "    y = ~est_diff,\n",
    "    data = items_with_est\n",
    "  )\n",
    "\n",
    "cat(\"Mean original difficulty: \", mean(items_with_est$diff), \"\\n\")\n",
    "cat(\"Mean estimated difficulty: \", mean(items_with_est$est_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly() %>%\n",
    "  add_markers(\n",
    "    x = ~skill,\n",
    "    y = ~est_skill,\n",
    "    data = students_with_est\n",
    "  )\n",
    "\n",
    "cat(\"Mean original skill: \", mean(students_with_est$skill), \"\\n\")\n",
    "cat(\"Mean estimated skill: \", mean(students_with_est$est_skill))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "We can see that the original and estimated values create a line quite well. But we can also observe a systematic difference between these values. Moreover, it changes each time we rerun our model fitting. To understand what happens we need to look at how our model is constructed:\n",
    "```\n",
    "...\n",
    "t_logit = t_skills_gathered - t_diffs_gathered\n",
    "...\n",
    "```\n",
    "\n",
    "It means that logits would remain the same if we would add or substract the same value from all difficulties and skill levels. Thus, when there is a single best solution: [skills, difficulties], the solution [skills + $\\alpha$, difficulties + $\\alpha$] is equally as good. Our model just finds one of these equally good solutions. And sometimes it can be something like [skills + 1000, difficulties + 1000]. When we need to fit our model for prediction only it's OK. But when we fit our model for the sake of obtaining interpretable parameters it may pose a problem. To counter it, we may set additional constraints, which will help us select a single solution out of the infinity. For IRT models, in such a situation it's safe to assume that the average skill in the population (or our sample) is equal 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    So, let's modify our model, so it will always look for solutions that will have mean of the skill estimations close to 0.<br/>\n",
    "    <b>Exercise:</b> replace NA's with operation nodes we need:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function building a computation graph up to the total loss;\n",
    "# it's similar to the previous one, but contains additional scaling\n",
    "#   components, which are intended to keep avg. skill at 0\n",
    "build_1pl_irt_model_scaled <- function(irt_data){\n",
    "    # a vector for storing items' difficulties\n",
    "    t_diffs = tf$Variable(\n",
    "        tf$random_normal(\n",
    "           shape = shape(length(levels(irt_data$item))),\n",
    "           mean = 0,\n",
    "           stddev = 6.0\n",
    "        ),\n",
    "        name = 'diffs'\n",
    "    )\n",
    "  \n",
    "    # a vector for storing students' skill levels\n",
    "    t_skills = tf$Variable(\n",
    "        tf$random_normal(\n",
    "            shape = shape(length(levels(irt_data$student))),\n",
    "            mean = 0,\n",
    "            stddev = 6.0\n",
    "        ),\n",
    "        name = 'skills'\n",
    "    )\n",
    "  \n",
    "    # vectors for storing difficulties and skill levels related for given observation\n",
    "    # irt_data$item_ind and irt_data$student_ind are implicitly loaded as constants into the computation graph\n",
    "    t_diffs_gathered = tf$gather(t_diffs, irt_data$item_ind - 1L)      # we substract 1, because in python indexes start from 0 \n",
    "    t_skills_gathered = tf$gather(t_skills, irt_data$student_ind - 1L)\n",
    "\n",
    "    # now we calculate logits and probabilities for each observed data point\n",
    "    t_logit = t_skills_gathered - t_diffs_gathered\n",
    "    prob = tf$sigmoid(t_logit)\n",
    "\n",
    "    # a constant for the observation of a correct or a wrong answer\n",
    "    y = tf$constant(as.double(irt_data$correct))\n",
    "\n",
    "    # mean of students' skills\n",
    "    skills_mean = NA # TODO: calculate the mean of t_skills\n",
    "    \n",
    "    # loss observed in a given row - cross-entropy\n",
    "    loss_each = tf$losses$log_loss(y, prob)\n",
    "    # loss averaged over all rows\n",
    "    loss = tf$reduce_mean(loss_each, name = 'loss') +\n",
    "               # it will be lowest when skills_mean will be equal 0\n",
    "               0.01 * NA # TODO: add the absolute value of the skills_mean to the loss function\n",
    "  \n",
    "    # we return some of the graph nodes, e.g. vectors with skill levels and difficulties\n",
    "    list(\n",
    "        t_diffs = t_diffs,\n",
    "        t_skills = t_skills,\n",
    "        t_diffs_gathered = t_diffs_gathered,\n",
    "        t_skills_gathered = t_skills_gathered,\n",
    "        skills_mean = skills_mean,\n",
    "        prob = prob,\n",
    "        loss = loss\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, let's build a model for current dataset one more time. This time we will use a function building an improved graph.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_1pl_irt_model_scaled(test_data)\n",
    "sess = tf$Session()\n",
    "model = add_training(model, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a number of backward and forward propagation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in 0:2000){\n",
    "    sess$run(model$train)\n",
    "    if(i %% 100 == 0){\n",
    "        cat(i, ') ', format(Sys.time(), \"%Y-%m-%d %X\"), ' Loss: ', sess$run(model$loss), ', Mean skill: ', sess$run(model$skills_mean), '\\n', sep = '')\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we extract estimated parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_with_est = cbind(items, est_diff = sess$run(model$t_diffs))\n",
    "students_with_est = cbind(students, est_skill = sess$run(model$t_skills))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can compare values estimated by the model with these originally assumed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly() %>%\n",
    "  add_markers(\n",
    "    x = ~diff,\n",
    "    y = ~est_diff,\n",
    "    data = items_with_est\n",
    "  )\n",
    "\n",
    "cat(\"Mean original difficulty: \", mean(items_with_est$diff), \"\\n\")\n",
    "cat(\"Mean estimated difficulty: \", mean(items_with_est$est_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly() %>%\n",
    "  add_markers(\n",
    "    x = ~skill,\n",
    "    y = ~est_skill,\n",
    "    data = students_with_est\n",
    "  )\n",
    "\n",
    "cat(\"Mean original skill: \", mean(students_with_est$skill), \"\\n\")\n",
    "cat(\"Mean estimated skill: \", mean(students_with_est$est_skill))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It seems the new model correctly centers skill levels close to 0</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Check model with empirical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real world, we don't know the true values of parameters. However, it is possible to check if our model works using empirical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 Group estimated probabilities into buckets and compare with CFT (correct on the first try)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare data. Observed data are: student id, item id, flag if a given student answer a given item correctly at the first try. Estimated data are: student id, item id, estimated difficulty for a given item, estimated skill for a given student. Based on estimated data we can calculate the probability of the correct answer on the first try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare data \n",
    "test_estimated_data <- test_data %>%\n",
    "  select(student, item, correct) %>% \n",
    "  inner_join(items_with_est %>% select(-diff), by = 'item') %>% \n",
    "  inner_join(students_with_est %>% select(-skill), by = 'student') %>%\n",
    "  mutate(calc_logit = est_skill - est_diff,\n",
    "         calc_prob = 1 / (1 + exp(-calc_logit))) # calculate probability of correct answer (CFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(test_estimated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's group estimated probabilities into buckets\n",
    "buckets <- test_estimated_data %>%\n",
    "  mutate(calc_prob_range = cut(calc_prob, breaks = (0:20)*0.05, include.lowest = TRUE, ordered_result = TRUE))\n",
    "\n",
    "head(buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For each bucket, we can look at the mean CFT and mean estimated probability of CFT and compare the results. These values for each bucket should be similar.<br>\n",
    "There is also the accuracy metric provided (the higher accuracy, the better estimation). However, point out that accuracy metric is higher for buckets closer to 0 or 1 but quite low for buckets close to 0.5. Does it seem to be reasonable? </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets %>%\n",
    "  group_by(calc_prob_range) %>%\n",
    "  summarise(\n",
    "    n = n(),\n",
    "    correct_perc = mean(correct),\n",
    "    correct_perc_calc = mean(calc_prob),\n",
    "    accuracy = mean((correct & calc_prob > 0.5) | (!correct & calc_prob < 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2 Calculate cross-entropy between estimation and observed value, compare results to baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare baselines. We calculate mean CFT for each item, mean CFT for each student, and mean CFT for a whole book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(test_estimated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines <- test_estimated_data %>% \n",
    "  group_by(item) %>% \n",
    "  mutate(item_mean_correct = mean(correct)) %>% \n",
    "  group_by(student) %>% \n",
    "  mutate(student_mean_correct = mean(correct)) %>% \n",
    "  ungroup() %>% \n",
    "  mutate(book_mean_correct = mean(correct))\n",
    "\n",
    "head(baselines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate cross entropy for observed values and our estimation from 1PL model and for observed values and our baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess <- tf$Session()\n",
    "\n",
    "# setting nodes\n",
    "tf_correct <- tf$constant(baselines$correct %>% as.numeric())\n",
    "\n",
    "tf_calc_prob <- tf$constant(baselines$calc_prob)\n",
    "tf_item_mean_correct <- tf$constant(baselines$item_mean_correct)\n",
    "tf_student_mean_correct <- tf$constant(baselines$student_mean_correct)\n",
    "tf_book_mean_correct <- tf$constant(baselines$book_mean_correct)\n",
    "tf_coin_flip <- tf$fill(list(tf$size(tf_correct)), tf$constant(0.5))\n",
    "\n",
    "# cross-entropy between tf_correct and estimations\n",
    "ce_calc_prob <- tf$losses$log_loss(labels = tf_correct, predictions = tf_calc_prob)\n",
    "ce_item_mean_correct  <- tf$losses$log_loss(labels = tf_correct, predictions = tf_item_mean_correct)\n",
    "ce_student_mean_correct <- tf$losses$log_loss(labels = tf_correct, predictions = tf_student_mean_correct)\n",
    "ce_book_mean_correct <- tf$losses$log_loss(labels = tf_correct, predictions = tf_book_mean_correct)\n",
    "ce_coin_flip <- tf$losses$log_loss(labels = tf_correct, predictions = tf_coin_flip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model (with the lowest cross entropy) should be model 1PL as it takes into account both: students' skills and items' difficulties. Mean CFT for each item and mean CFT for each student should have higher cross entropy than 1PL. The worst 'model' should be a coin flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(sprintf('Cross entropy for observed values compared with:\n",
    "- values estimated by 1PL model: %f, \n",
    "- mean CFT of each item: %f, \n",
    "- mean CFT of each student: %f, \n",
    "- mean CFT of whole book: %f, \n",
    "- coin flip: %f', \n",
    "sess$run(ce_calc_prob), sess$run(ce_item_mean_correct), sess$run(ce_student_mean_correct), \n",
    "sess$run(ce_book_mean_correct), sess$run(ce_coin_flip)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Eager execution in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively to the logic described above, a new mode called the _eager execution_ was recently introduced to the TensorFlow environment. Using this mode, we don't need to run computations in sessions. Instead, all calculations are done immediately. Working in this mode is more comfortable for experimenting, e.g. it lets debug code much more easily, but it's less efficient than a session-based mode when we put such code on production. To read more about the _eager execution_ mode you can go to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [8 Things To Do Differently in Tensorflow’s Eager Execution Mode](https://medium.com/coinmonks/8-things-to-do-differently-in-tensorflows-eager-execution-mode-47cf429aa3ad)\n",
    "- [Eager Execution](https://www.tensorflow.org/guide/eager) - TensorFlow documentation\n",
    "- [More flexible models with TensorFlow eager execution and Keras](https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup/) - TensorFlow for R Blog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Probabilistic programming tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Probabilistic programming tools let us easily estimate parameter values not only as most likely point estimates, but as so-called posterior distributions. These distributions let us for example find out how accurate our estimates are.</p>\n",
    "<p>E.g. let's assume that MLE of a parameter is equal 2.0. Having only that information, we don't know how accurate it is. But, when we have a distribution of a parameter likelihood, we can check the accuracy of the estimation, (e.g. how much information we were able to extract from the data). When an interval containing 95% of most likely parameter values is narrow, e.g $[1.99; 2.01]$, then we know our estimates are precise. But, when it is very wide, e.g. $[-98; 102]$, we know it isn't accurate.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [greta](https://greta-stats.org) - a package that lets us specify a statistical model directly in the R code. It's built on top of TensorFlow, so it can seamlessly use a GPGPU when it's available in the system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
