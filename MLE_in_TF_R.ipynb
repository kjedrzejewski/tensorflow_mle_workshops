{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find the documentation at:\n",
    "\n",
    "- [TenorFlow in R](https://tensorflow.rstudio.com) - The R package we're using\n",
    "- [TensorFlow Python API](https://www.tensorflow.org/api_docs/python/tf) - TensorFlow itself (the documentation is for a Python library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Computation Graphs and Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, let's load the packages we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Sessions represent a connection between the client program and the C++ runtime. We need them to run all computations, e.g. because they store values of variables. Let's create one:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf$Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we finish working with a graph, we should close the session, to free up the resources associated with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, since we will need a session, let's create the next one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf$Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Computation graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Computations in TensorFlow are expressed as Computation Graphs, which consist of multiple types of nodes:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - constants\n",
    "<p>Constants are tensors the values of which cannot be changed after their creation. We can use them to store observation data, esp. if we would like to estimate our model parameters using General Purpose Graphics Processing Units (GPGPUs). This way we will avoid continuous data transfer between GPGPU and the main computer memory.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnst = tf$constant(42) # let's define a constant which contains the answer to life, the universe and everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>sess$run(var)</b> means \"evaluate and return the value of a node which is indicated by the variable <b>var</b>\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(cnst) # and now let's read its value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Constants and other tensors do not need to be scalars. They can be vectors or matrices having 2 or more dimensions<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = 1:10\n",
    "cnst_vec = tf$constant(vec) # a constant containing a vector\n",
    "sess$run(cnst_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = matrix(1:16, 4, 4)\n",
    "cnst_mat = tf$constant(mat) # a constant containing a 2D matrix\n",
    "sess$run(cnst_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - variables\n",
    "<p>Variables are tensors the values of which can be changed when operations are run. They can be initialised using a constant or a random value from a selected distribution. We can use them to represent model parameters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrbl = tf$Variable(                                           # let's create a variable\n",
    "    tf$random_normal(shape = shape(1), mean = 0, stddev = 1)  # that will be a single scalar, initialised from normal(0,1) distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(tf$global_variables_initializer()) # let's initialise our variable (it needs to be run, even if we initialize a variable with a constant)\n",
    "# because, we initialise this variable using a random distribution, its value will change every time we will rerun this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(vrbl) # and check its value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - placeholders\n",
    "<p>Placeholders are tensors that we can use to pass values in the moment of running a graph. When we define a placeholder, we need to provide its values at the moment of running a node, which either is that placeholder, or which depends directly or indirectly on that placeholder. In general, they behave similarly to constants, but when we use them, their values need to be transferred to the memory of the processing unit that is running the graph. It's not a big deal if the processing is done on a local CPU â€“ but it will slow down computations run on the GPU.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phld = tf$placeholder(tf$float32) # let's define a placeholder, which will expect to have a 32-bit float (not double) assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(phld) # when we run a graph that contains placeholders without setting their values, it throws an error, because the value of a placeholder isn't set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(phld, feed_dict = dict(phld = 8)) # when we set a value to a placeholder when running a graph that contains it, its value will be returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - operations\n",
    "<p>Operations are nodes that do the actual calculations. They take tensors as inputs and produce tensors as outputs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf$constant(1L) # let's define two constant nodes...\n",
    "b = tf$constant(2L)\n",
    "\n",
    "c = a + b # ... and a node which is equal to their sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_2_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(c) # yep, it's confirmed, 1 + 2 = 3. Math in TensorFlow works like anywhere else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Example - Pythagorean theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this example, we will create a graph that uses the Pythagorean theorem to calculate the length of the <i>hypotenuse</i> when having the lenghts of <i>legs</i> provided as inputs. The equation we need is:</p>\n",
    "<p>$c = \\sqrt{a^2 + b^2}$</p>\n",
    "<p>Let's express it as a computation graph:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf$placeholder(tf$float32) # placeholder for two sides lengths\n",
    "b = tf$placeholder(tf$float32)\n",
    "\n",
    "c_square = a^2 + b^2  # now we need to calculate sum of legs lengths squares (or rather create a node that computes it)\n",
    "\n",
    "c = tf$sqrt(c_square) # and now we can calculate the square root of c_square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The graph we created above looks like:</p>\n",
    "\n",
    "![](img/1_3_pyth_graph.png)\n",
    "\n",
    "<p>Now we can evaluate the <i>c</i> node value in a session:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(c, feed_dict = dict(a = 3, b = 4)) # because a and b are placeholders, and they're needed for c node calculation\n",
    "                                            # we need to assign a and b values when running the c node calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(c(a = a, b = b, c_2 = c_square, c = c), feed_dict = dict(a = 3, b = 4)) # of course, we can also run multiple nodes in one call.\n",
    "                                                                                 # then value of each node is returned in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Example - vector cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we will create a graph that will calculate the value of a cosine between two vectors. The equation we will use is:</p>\n",
    "$\\cos(âˆ¡\\mathbf{A}\\mathbf{B}) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i  B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}  \\sqrt{\\sum\\limits_{i=1}^{n}{B_i^2}} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_a = tf$placeholder(tf$float64)\n",
    "vec_b = tf$placeholder(tf$float64)\n",
    "\n",
    "dotprod_ab = tf$tensordot(vec_a, vec_b, 1L) # 1L as we need to pass an integer 1\n",
    "len_a = tf$norm(vec_a)\n",
    "len_b = tf$norm(vec_b)\n",
    "\n",
    "len_ab_prod = len_a * len_b\n",
    "\n",
    "cos_ab = dotprod_ab / len_ab_prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_4_cosine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(cos_ab, feed_dict = dict(vec_a = c(3,4,5,6), vec_b = c(6,8,10,12))) # the result here should be 1, because\n",
    "                                                                             # these vectors have exactly the same direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(cos_ab, feed_dict = dict(vec_a = c(3,4), vec_b = c(-4,3))) # the result here should be 0, because these vectors are perpendicular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(cos_ab, feed_dict = dict(vec_a = c(3,4,10), vec_b = c(-3,-4,-10))) # the result here should be -1, because a points in the opposite direction than b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Build a graph which will calculate the value of covariance between <i>x</i> and <i>y</i> vectors which should be provided as placeholders' values during a session$run() call.</p>\n",
    "\n",
    "$cov(x,y) = {\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y}) \\over n-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = NA # TODO: define a placeholder for x\n",
    "y = NA # TODO: define a placeholder for y\n",
    "\n",
    "mean_x = NA # TODO: calculate the mean of x\n",
    "mean_y = NA # TODO: calculate the mean of y\n",
    "\n",
    "n_1 = tf$cast(tf$size(x), tf$float64) - 1 # number of elements in x minus 1\n",
    "\n",
    "diff_x = NA # TODO: calculate the difference between vector x and its mean\n",
    "diff_y = NA # TODO: calculate the difference between vector y and its mean\n",
    "\n",
    "diff_xy_prod = NA # TODO: calculate the dot product of diff_x and diff_y. You can use tf$reduce_sum or tf$tensordot\n",
    "\n",
    "cov_xy = NA # TODO: divide diff_xy_prod by n - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(cov_xy, feed_dict = dict(x = c(5,20,40), y = c(10,24,33)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The expected result is: <i>199.166666666667</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Loss function is used to measure how well a given model performs in terms of predicting the expected value. Various loss functions compare predicted and expected values differently. The most popular loss functions are MSE, MAE and cross-entropy. \n",
    "    <br>\n",
    "    <b>MSE</b> or <b>MAE</b> are used when a specific value is estimated (e.g. height in cm). \n",
    "<br>\n",
    "    <b>Cross-entropy</b> is used when a probability is estimated (e.g. probability that a student will answer a question correctly).   \n",
    "</p>\n",
    "<p><i>Usually name 'loss function' is used for a single observation and 'cost function' for an entire dataset. However, in tensorflow environment name 'loss function' is used for entire dataset as well. </i>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Example - MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> MSE - Mean Squared Error </b> is a loss function represented by the following equation:</p> \n",
    "$MSE = {1 \\over{n}} \\sum\\limits_{i=1}^{n}{(\\overline{y_i} - y_i)^2}$\n",
    "<br>\n",
    "where <br>\n",
    "$n$ - number of observations<br>\n",
    "$\\overline{y_i}$ - value predicted by the model for observation $i$ <br>\n",
    "$y_i$ - actual value for observation $i$ <br>\n",
    "\n",
    "<p> In this example, we will:\n",
    "    <ul>\n",
    "        <li> implement MSE from scratch in plain R, </li>\n",
    "        <li> use already implemented MSE in tensorflow.</li>\n",
    "    </ul>\n",
    "</p>     \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data\n",
    "set.seed(1234)\n",
    "y <- 1:10 \n",
    "(y_ <- y + rnorm(10, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE from scratch in R\n",
    "MSE <- mean((y_-y)^2)\n",
    "sprintf('MSE calculated from scratch in R: %.5f', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressing data as nodes\n",
    "ty <- tf$constant(y, dtype = 'float32') # constant - it is real value\n",
    "ty_ <- tf$constant(y_, dtype = 'float32') # now it's a constant as we only do math operations; \n",
    "                                          # however, usually it's Variable (changes over time during training)\n",
    "\n",
    "# MSE already implemented in tensorflow\n",
    "MSE_tf <- tf$losses$mean_squared_error(labels = ty, predictions = ty_)\n",
    "sprintf('MSE already implemented in tensorflow: %.5f', sess$run(MSE_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Exercise - MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> MAE - Mean Absolute Error </b> is a loss function represented by the following equation:</p> \n",
    "$MAE = {1 \\over{n}} \\sum\\limits_{i=1}^{n}{\\mid \\overline{y_i} - y_i \\mid}$\n",
    "<br>\n",
    "where <br>\n",
    "$n$ - number of observations<br>\n",
    "$\\overline{y_i}$ - value predicted by the model for observation $i$ <br>\n",
    "$y_i$ - actual value for observation $i$ <br>\n",
    "<br>\n",
    "<p> In this exercise, try to:\n",
    "    <ul>\n",
    "        <li> implement MAE from scratch in plain R, </li>\n",
    "        <li> use already implemented MAE in tensorflow</li>\n",
    "    </ul>\n",
    "    as it was done in the example above.\n",
    "</p>     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data\n",
    "set.seed(1234)\n",
    "y <- 1:10 \n",
    "(y_ <- y + rnorm(10, 0, 1))\n",
    "\n",
    "# Expressing data as nodes\n",
    "ty <- tf$constant(y, dtype = 'float32') # constant - it is real value\n",
    "ty_ <- tf$constant(y_, dtype = 'float32') # now it's a constant as we only do math operations; \n",
    "                                          # however, usually it's Variable (changes over time during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE <- NA # TODO: calculate MAE in plain R\n",
    "sprintf('MSE calculated from scratch in R: %.5f', MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_tf <- NA # TODO: find appropriate function already implemented in tensorflow \n",
    "sprintf('MAE already implemented in tensorflow: %.5f', sess$run(MAE_tf)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected value for each run is: $0.84257$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Example - cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> Cross-entropy </b> is a loss function represented by the following equation:</p> \n",
    "$cross-entropy = -{1 \\over{n}} \\sum\\limits_{i=1}^{n} {y_i \\cdot log \\overline{y_i} + (1 - y_i) \\cdot log (1 - \\overline{y_i})}$\n",
    "<br>\n",
    "where <br>\n",
    "$n$ - number of observations<br>\n",
    "$\\overline{y_i}$ - probability predicted by the model for observation $i$ <br>\n",
    "$y_i$ - actual outcome for observation $i$ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty <- tf$constant(rep(c(1,0), 5))\n",
    "ty_ <- tf$random_uniform(shape(10), minval = 0, maxval = 1, seed = 12345)\n",
    "\n",
    "sess$run(ty)\n",
    "sess$run(ty_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce <- tf$losses$log_loss(labels = ty, predictions = ty_)\n",
    "sess$run(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The goal of training the model is to find the best fitting parameters. Loss function is used to evaluate model performance. To find the best estimators, we need to minimize this loss. Gradient descent algorithm is known as the first-order iterative optimizer. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent rule\n",
    "\n",
    "Updating parameters $\\vec{\\theta} = [{\\theta}_0, {\\theta}_1, ..., {\\theta}_k] $ in iterations:\n",
    "<br><br>\n",
    "${\\theta}_i := \\theta_i - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_i} loss(\\vec{\\theta}, X)$\n",
    "\n",
    "where <br>\n",
    "${\\theta}_i$ - $i$ parameter <br>\n",
    "$\\alpha$ - learning rate (hyperparameter) <br>\n",
    "$\\frac{\\partial}{\\partial \\theta_i} loss(\\vec{\\theta}, X)$ - partial derivative of loss function for parameter ${\\theta}_i$<br>\n",
    "$X$ - observations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "![](img/2_2_Gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Example - one number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We can implement gradient descent from scratch in plain R. To do it, we need to calculate the derivative of our loss function.\n",
    "Let's assume that:\n",
    "    <ul>\n",
    "        <li>we use MSE loss function,</li>\n",
    "        <li>we have only one training example ($n = 1$),</li>\n",
    "        <li>$\\overline{y} = \\theta_0$.</li> \n",
    "       </ul>\n",
    "Then $MSE = {1 \\over{n}} \\sum\\limits_{i=1}^{n}{(\\overline{y_i} - y_i)^2}$ for $n = 1$ is:\n",
    "    <br>\n",
    "    $ MSE = ( \\overline{y_1} - y_1)^2$ \n",
    "    <br>\n",
    "    and \n",
    "    <br>\n",
    "    $ \\frac{\\partial}{\\partial \\theta_0 } MSE(\\theta_0) = 2 \\cdot (\\overline {y_1} - y_1)$\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y <- 10 # assigning real value \n",
    "y_ <- 15 # assigning initial value of estimated parameter\n",
    "\n",
    "for (i in 1:10){\n",
    "  dy_ <- 2*(y_-y) # calculating derivative of loss function \n",
    "  y_ <- y_ - 0.1 * dy_ # updating estimated value according to gradient descent with learning rate = 0.1\n",
    "  print(y_)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent optimizer is already implemented in tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty <- tf$constant(10, dtype = 'float32') # real value \n",
    "ty_ <- tf$Variable(15, dtype = 'float32') # variable which we estimate; generally it's an operation node \n",
    "                                          # with result that depends on some variables, and the optimization process\n",
    "                                          # optimizes values of these variables\n",
    "sess$run(tf$variables_initializer(list(ty_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer <- tf$train$GradientDescentOptimizer(learning_rate = 0.1) # defining optimizer with learning rate = 0.1\n",
    "loss <- tf$losses$mean_squared_error(labels = ty, predictions = ty_) # defining loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i in 1:10){\n",
    "  train <- optimizer$minimize(loss) # updating values with gradient descent optimizer which minimize MSE loss function\n",
    "  sess$run(train)\n",
    "  print(sess$run(ty_))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Example - linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let us use more complex example to show how gradient descent works. Our task is to estimate $a, b$ and $c$ parameters in a linear model: \n",
    "<br>\n",
    "$z = a \\cdot x + b \\cdot y + c $  \n",
    "using gradient descent to minimize MSE loss function. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_222 <- readr::read_csv('data/example_222.csv') # read data\n",
    "head(example_222) # look at the data \n",
    "\n",
    "tx <- tf$constant(example_222$x, dtype = 'float32') # create node \n",
    "ty <- tf$constant(example_222$y, dtype = 'float32') # create node\n",
    "tz <- tf$constant(example_222$z, dtype = 'float32') # create node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta <- tf$Variable(2) # init parameters\n",
    "tb <- tf$Variable(4) # init parameters\n",
    "tc <- tf$Variable(-14) # init parameters\n",
    "sess$run(tf$global_variables_initializer())\n",
    "\n",
    "tz_ <- ta*tx + tb*ty + tc # create node to compute estimation of z \n",
    "\n",
    "# MODEL \n",
    "optimizer <- tf$train$GradientDescentOptimizer(learning_rate = .005) # choose Gradient Descent as optimizer\n",
    "MSE <- tf$losses$mean_squared_error(labels = tz, predictions = tz_) # choose MSE as loss function\n",
    "train <- optimizer$minimize(MSE) # minimizing MSE with Gradient Descent optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter <- 100 # number of iterations in training \n",
    "\n",
    "# helper table to save results\n",
    "results <- tibble::tibble(\n",
    "  it = 0:iter,  \n",
    "  mse = c(sess$run(MSE), rep(NA_real_, iter)),\n",
    "  a = c(sess$run(ta), rep(NA_real_, iter)),\n",
    "  b = c(sess$run(tb), rep(NA_real_, iter)),\n",
    "  c = c(sess$run(tc), rep(NA_real_, iter)))\n",
    "\n",
    "results[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING \n",
    "for (i in 1:iter){\n",
    "\n",
    "  sess$run(train) # one training\n",
    "  \n",
    "  # save results\n",
    "  results$it[i+1] <- i\n",
    "  results$mse[i+1] <- sess$run(MSE)\n",
    "  results$a[i+1] <- sess$run(ta) \n",
    "  results$b[i+1] <- sess$run(tb)\n",
    "  results$c[i+1] <- sess$run(tc)    \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check results\n",
    "head(results)\n",
    "tail(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(magrittr)\n",
    "library(plotly)\n",
    "\n",
    "# Let's plot how loss has changed during training\n",
    "results %>% \n",
    "  plot_ly(x=~it, y=~mse) %>% \n",
    "  add_lines() %>% \n",
    "  layout(xaxis = list(title = 'Iteration'), \n",
    "         yaxis = list(title = 'Loss (mse)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot how parameters have changed during training\n",
    "results %>% \n",
    "  tidyr::gather(key, Values, -c(it, mse)) %>% \n",
    "  plot_ly(x = ~it, y = ~ Values, color = ~key) %>% \n",
    "  add_lines() %>% \n",
    "  layout(xaxis = list(title = 'Iteration')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Exercise - lasso regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> Linear regression model </b> is a model which tries to fit the best straight line to the data by minimizing MSE. \n",
    "    <br>\n",
    "    <b> Lasso </b> and <b> ridge linear regression models </b> are models which also try to fit the best straight line to the data, but by <b>minimizing MSE plus some additional penalty component</b>.\n",
    "    <br> \n",
    "    <br>\n",
    "    For linear model: $z = a \\cdot x + b \\cdot y + c $, <br>\n",
    "    where $a, b, c$ are parameters  \n",
    "    <ul>\n",
    "        <li>ridge regression minimizes: $MSE + \\lambda \\cdot (a^2 + b^2)$ </li>\n",
    "        <li>lasso regression minimizes: $MSE + \\lambda \\cdot (\\mid a \\mid  + \\mid b \\mid)$ </li>\n",
    "    </ul> \n",
    "    where $\\lambda$ is a hyperparameter which termines how severe the additional penalty is.\n",
    "</p>\n",
    "<br>\n",
    "    More details about ridge and lasso regressions in videos:\n",
    "\n",
    "- [Ridge Regression](https://www.youtube.com/watch?v=Q81RR3yKn30)\n",
    "- [Lasso Regression](https://www.youtube.com/watch?v=NGf0voTMlcs) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you know how loss functions for ridge and lasso regression models look like, try to implement <b>lasso regression </b>on data from the example above (2.2.2 Example - linear regression).\n",
    "<br> \n",
    "<br>\n",
    "The task is to estimate $a, b$ and $c$ parameters in $z = a \\cdot x + b \\cdot y + c$   model using lasso regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data and creating nodes\n",
    "\n",
    "example_222 <- readr::read_csv('data/example_222.csv') # read data\n",
    "head(example_222) # look at the data \n",
    "tx <- tf$constant(example_222$x, dtype = 'float32') # create node \n",
    "ty <- tf$constant(example_222$y, dtype = 'float32') # create node\n",
    "tz <- tf$constant(example_222$z, dtype = 'float32') # create node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta <- NA # TODO: init parameter a=2\n",
    "tb <- NA # TODO: init parameter b=4\n",
    "tc <- NA # TODO: init parameter c=-14\n",
    "\n",
    "sess$run(tf$global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_ <- NA # TODO: create node for z (node with equation which represents linear model)\n",
    "\n",
    "optimizer <- NA # TODO: select gradient descent optimizer and set learning rate=0.005\n",
    "mse <- NA # TODO: select mse loss function\n",
    "loss <- NA # TODO: create whole loss function: to mse add additional component for lasso regression and lambda = 0.2\n",
    "train <- NA # TODO: minimize loss function\n",
    "\n",
    "model <- list(mse = mse, \n",
    "             loss = loss, \n",
    "             optimizer = optimizer, \n",
    "             train = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter <- 100\n",
    "\n",
    "results <- tibble::tibble(\n",
    "  it = 0:iter,  \n",
    "  mse = c(sess$run(mse), rep(NA_real_, iter)),\n",
    "  loss = c(sess$run(loss), rep(NA_real_, iter)),  \n",
    "  a = c(sess$run(ta), rep(NA_real_, iter)),\n",
    "  b = c(sess$run(tb), rep(NA_real_, iter)),\n",
    "  c = c(sess$run(tc), rep(NA_real_, iter)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i in 1:iter){\n",
    "\n",
    "  NA # TODO: run a single training iteration\n",
    "  \n",
    " # save results\n",
    "  results$it[i+1] <- i\n",
    "  results$mse[i+1] <- sess$run(mse)\n",
    "  results$loss[i+1] <- sess$run(loss)\n",
    "  results$a[i+1] <- sess$run(ta) \n",
    "  results$b[i+1] <- sess$run(tb)\n",
    "  results$c[i+1] <- sess$run(tc)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(results)\n",
    "tail(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Other optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent algorithm is an example of optimizers. However, there are also others gradient descent based optimizers:\n",
    "<ul> \n",
    "    <li><b>[Momentum optimizer](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer)</b> - helps accelerate gradient descent in the right direction and decrease oscillations by keeping the momentum.\n",
    "    <li><b>[RMSProp optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer)</b> - has the same aim as momentum optimizer, however there is a difference in formula.\n",
    "    </li>\n",
    "    <li><b>[Adam optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)</b> - mixes Momentum and RMSProp approaches</li>\n",
    "    <li> ...</li>\n",
    "    </ul>\n",
    "Let's see how to call different optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expressing data in nodes\n",
    "ty <- tf$constant(10, dtype = 'float32') # real value \n",
    "ty_ <- tf$Variable(15, dtype = 'float32') # variable which we estimate; generally it's an operation node \n",
    "\n",
    "loss <- tf$losses$mean_squared_error(labels = ty, predictions = ty_) # loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer <- tf$train$GradientDescentOptimizer(learning_rate = 0.005) # gradient descent optimizer\n",
    "train <- optimizer$minimize(loss) # train \n",
    "\n",
    "sess$run(tf$global_variables_initializer()) # init variables\n",
    "\n",
    "# Gradient descent\n",
    "for (i in 1:10)\n",
    "{sess$run(train)\n",
    "  print(sess$run(ty_))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer <- tf$train$MomentumOptimizer(learning_rate = 0.005, momentum = 0.8) # momentum optimizer\n",
    "train <- optimizer$minimize(loss) # train \n",
    "\n",
    "sess$run(tf$global_variables_initializer()) # init variables\n",
    "\n",
    "# Momentum optimizer\n",
    "for (i in 1:10)\n",
    "{sess$run(train)\n",
    "  print(sess$run(ty_))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Pythagorean theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We know the values of $b=12$ and $c=13$ and we know that $a,b$ and $c$ satisfy the Pythagorean theorem:  $c = \\sqrt{a^2 + b^2}$.\n",
    "Let's assume that we are not able to transform this equation to calculate $a$ value. \n",
    "    <br> The task is to estimate $a$ when we know $b$ and $c$ values and Pythagorean theorem.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_b <- NA # TODO: create node with b = 12\n",
    "tf_c <- NA # TODO: create node with c = 13\n",
    "tf_a <- NA # TODO: create node for a; initialize it with any value you want\n",
    "\n",
    "c_ <- NA # TODO: create node with c_ which follows the Pythagorean theorem\n",
    "\n",
    "loss <- NA # TODO: as loss function choose MSE\n",
    "optimizer <- NA # TODO: as optimizer choose Adam optimizer, choose the learning rate \n",
    "train <- NA # TODO: set optimizer to minimize loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA # TODO: initialize all variables\n",
    "\n",
    "print(paste0('current loss: ', sess$run(loss))) # print current value of loss function for randomly initialized value of a\n",
    "\n",
    "NA # TODO: do one iteration of training\n",
    "\n",
    "print(paste0('loss after one iteration of training: ', sess$run(loss))) # print current value of loss function after one iteration\n",
    "print(paste0('value of a after one iteration of training: ', sess$run(tf_a))) # print current value of a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results <- tibble::tibble(current_loss = sess$run(loss), \n",
    "                   current_value = sess$run(tf_a))\n",
    "                   \n",
    "for (i in 1:200){\n",
    "\n",
    "  NA # TODO:  do one iteration of training\n",
    "\n",
    "  # save results to training table\n",
    "  results <- results %>% \n",
    "    rbind(tibble::tibble(current_loss = sess$run(loss), \n",
    "                 current_value = sess$run(tf_a)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot how loss and parameter value have changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results %>% \n",
    "  plot_ly(x = 1:nrow(.), y = ~current_loss) %>% \n",
    "  add_lines() %>% \n",
    "  layout(xaxis = list(title = 'Iteration'), \n",
    "         yaxis = list(title = 'Loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results %>% \n",
    "  plot_ly(x = 1:nrow(.), y = ~current_value) %>% \n",
    "  add_lines() %>% \n",
    "  layout(xaxis = list(title = 'Iteration'), \n",
    "         yaxis = list(title = 'Value of a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Times table (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 different numbers. We don't know their values. However, we know the products of all combinations of those numbers (100 products). The goal is to estimate values of those 10 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_table <- readr::read_csv('data/example_times_table.csv') %>% # read data\n",
    "    mutate(\n",
    "        a_id = as.integer(a_id),\n",
    "        b_id = as.integer(b_id)\n",
    "    )\n",
    "head(times_table) # look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read data: \n",
    "- The first number squared equals 4\n",
    "- Product of the second and the first numbers equals 6\n",
    "- Product of the third and the first numbers equals 10 \n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_indices <- NA # TODO: create node for a indices (from times_table) with dtype = 'int32'\n",
    "b_indices <- NA # TODO: create node for b indices (from times_table) with dtype = 'int32'\n",
    "y <- NA # TODO: create node for a product (from times_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values <- NA # TODO: create node for values of parameters which we'll estimate,\n",
    "            # we'll estimate 10 numbers (length(unique(times_table$a_id)))\n",
    "            # init values from uniform distribution [1,10]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to match indices with their values (that is: each factor ($a$ and $b$) with index 0 is matched with the first value from tensor <b>values</b>, each factor ($a$ and $b$) with index 1 is matched with the second value from tensor <b>values</b> and so on). To do it, we use function <b>gather</b>. [Here more information about gather.](https://www.tensorflow.org/api_docs/python/tf/gather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_a_gathered <- tf$gather(values, a_indices) # gather a parameters with indices\n",
    "t_b_gathered <- tf$gather(values, b_indices) # gather b parameters with indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ <- NA # TODO: create node which is a product of a and b; NOTE: you need to use gathered values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL \n",
    "loss <- NA # TODO: as loss function choose MSE\n",
    "optimizer <- NA # TODO: as optimizer choose Adam optimizer, choose the learning rate \n",
    "train <- NA # TODO: set optimizer to minimize loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA # TODO: init all variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tibble)\n",
    "# let us take a look at the initialized values of t_a_gathered, t_b_gathered and y_\n",
    "print('a values:')\n",
    "sess$run(t_a_gathered)\n",
    "print('b values:')\n",
    "sess$run(t_b_gathered)\n",
    "print('products for iteration 0:')\n",
    "sess$run(y_)\n",
    "\n",
    "# Let us check what is a loss for randomly initialized values\n",
    "print(paste0('loss for iteration 0: ', sess$run(loss)))\n",
    "\n",
    "# Let us prepare a helper table losses with step number and loss value\n",
    "step = 0\n",
    "losses = tibble(\n",
    "    step = step,\n",
    "    current_loss = sess$run(loss)\n",
    ")\n",
    "\n",
    "# Let us prepare a helper table params with step number, values of parameters for each index in a given estimation step, \n",
    "# and indices 0:9 for each value (just to plot it later)\n",
    "params <- tibble(step = step,\n",
    "                 values_calc = sess$run(values),\n",
    "                 id = 0:9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN \n",
    "\n",
    "for (i in 1:500) {\n",
    "    \n",
    "    NA # TODO: one iteration of training\n",
    "    \n",
    "    losses = rbind(\n",
    "        losses,\n",
    "        tibble(step = step + i,\n",
    "               current_loss = sess$run(loss))\n",
    "    )\n",
    "    \n",
    "    params = rbind(\n",
    "        params, \n",
    "        tibble(step = step,\n",
    "                 values_calc = sess$run(values),\n",
    "                 id = 0:9)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check our results\n",
    "tail(losses, 5)\n",
    "tail(params, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot loss function\n",
    "\n",
    "losses %>% \n",
    "  plot_ly(x = ~step, y = ~current_loss) %>% \n",
    "  add_lines() %>% \n",
    "  layout(xaxis = list(title = 'Iteration'), \n",
    "         yaxis = list(title = 'Loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <b>Likelihood</b> measures the plausibility of a model parameter value, given observations we have.<br/>\n",
    "    $\\mathcal{L}(\\theta \\mid x) = p_\\theta (x) = P (X=x; \\theta)$, where\n",
    "    <ul>\n",
    "        <li>$\\mathcal{L}(\\theta \\mid x)$ is the likelihood values</li>\n",
    "        <li>$p_\\theta (x) = P (X=x; \\theta)$ is the probability of observing $x$ given parameter values are equal $\\theta$</li>\n",
    "        <li>$x$ observed data</li>\n",
    "        <li>$\\theta$ parameter values</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "    In other words, it tells us what is the probability of observing what we observed, assuming the given model parameter values. The value of this probability changes depending on the $\\theta$. When these parameters have values that do not let $x$ occur, $P (X=x; \\theta) = 0$, and when it has values in which $x$ must always occur $P (X=x; \\theta) = 1$.\n",
    "</p>\n",
    "<p>\n",
    "    Of course, we usually have more than one observation: $x_1$, $x_2$, $x_3$, ... So it's likelihood is equal to:<br/>\n",
    "    $\\mathcal{L}(\\theta \\mid x_1, x_2, ..., x_n) = P (X=[x_1, x_2, ..., x_n]; \\theta) = \\prod_{i=1}^n P (X=x_i; \\theta)$<br/>\n",
    "</p>\n",
    "<p>\n",
    "    The $\\theta$ for which the value of $\\prod_{i=1}^n P (X=x_i; \\theta)$ is highest is called the Maximum Likelihood Estimate, and in the frequentist statistics it is assumed to be an accurate estimation of a latent parameter (a parameter that can't be measured directly).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Cross-entropy and MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Now, let's take the equation from earlier<br/>\n",
    "    $\\mathcal{L}(\\theta \\mid x_1, x_2, ..., x_n) = \\prod_{i=1}^n P (X=x_i; \\theta)$\n",
    "</p>\n",
    "<p>\n",
    "    and logarithmise both sides of this equation:<br/>\n",
    "    $log \\mathcal{L}(\\theta \\mid x_1, x_2, ..., x_n) = log\\prod_{i=1}^n P (X=x_i; \\theta) = \\sum_{i=1}^n log P (X=x_i; \\theta)$<br/>\n",
    "    logarithm of the likelihood is called the log-likelihood and has the same maximum as the likelihood because logarithm is a monotonically increasing function.\n",
    "</p>\n",
    "<p>\n",
    "    Now, let's assume that we have a probability that something happened ($y_i = 1$), e.g. a student answered a test question correctly $P (Y=1; \\theta)$, then the probability of that not occuring ($y_i = 0$) in the same conditions - $P (Y=0; \\theta)$ - is equal to: $1 - P (Y=1; \\theta)$. Then:<br/>\n",
    "    $log \\mathcal{L}(\\theta \\mid y_i) = log P (Y=y_i; \\theta) = y_i \\cdot log P(Y=1; \\theta) + (1 - y_i) \\cdot log (1 - P(Y=1; \\theta))$\n",
    "</p>\n",
    "<p>\n",
    "    For simplification, let's denote $P(Y=1; \\theta)$ for given $y_i$ as $p_{i,\\theta}$. Now:<br/>\n",
    "    $log \\mathcal{L}(\\theta \\mid y_i) = y_i \\cdot log p_{i,\\theta} + (1 - y_i) \\cdot log (1 - p_{i,\\theta})$\n",
    "</p>\n",
    "<p>\n",
    "    And now let's multiply both sides by $-1$:<br/>\n",
    "    $-log \\mathcal{L}(\\theta \\mid y_i) = -(y_i \\cdot log p_{i,\\theta} + (1 - y_i) \\cdot log (1 - p_{i,\\theta})) = cross$-$entropy(p_{i,\\theta}, y_i)$\n",
    "</p>\n",
    "<p>\n",
    "    Thus:\n",
    "    <ul>\n",
    "        <li><b>when we minimise the value of cross-entropy we also maximise the likelihood</b> (because we minimise the negative log-likelihood)</li>\n",
    "        <li>what we do in TensorFlow is not magic that just works, it's a well established statistical method that has been in use for over a century</li>\n",
    "        <li>it also applies to very complicated models called neural networks. Just in this case, we do not believe that parameters we estimate make any sense</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Example - an unbalanced coin and MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages we will need\n",
    "library(tensorflow)\n",
    "library(tidyverse)\n",
    "library(plotly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will use TF and MLE to estimate the probability that an unbalanced coin lands heads up - $p_H$. Below we have a result of 100 coin tosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - HEADS, 0 - TAILS\n",
    "results = c(0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p = tf$Variable(0.01) # the variable containing a probability of a coin landing heads up. Let's initialise\n",
    "                        # it with 0.01 (assume this coin almost always lands tails up)\n",
    "t_y = tf$constant(results)\n",
    "\n",
    "p_vector = tf$fill(list(tf$size(t_y)), t_p) # we need to calculate the expected probability for each coin toss we observed\n",
    "                                            # however, it's the same in all attempts, so we just assume the current t_p estimate for each observation\n",
    "\n",
    "t_loss = tf$losses$log_loss(t_y, p_vector) # now we need to calculate the cross-entropy between estimated probability, and each observation\n",
    "loss = tf$reduce_mean(t_loss)              # here we average losses for all observation, to have a single value, which we will want to minimise\n",
    "\n",
    "# and now, we also need to establish the optimisation process...\n",
    "optimizer = tf$train$AdamOptimizer(0.1)\n",
    "train = optimizer$minimize(loss)\n",
    "\n",
    "# as always we need to create a session\n",
    "sess = tf$Session() \n",
    "sess$run(tf$global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 1000 # number of iterations to run\n",
    "\n",
    "# table for storing model parameter optimisation progress log\n",
    "r = tibble(\n",
    "    it = 1:iter,\n",
    "    loss = rep(NA_real_, iter),\n",
    "    prob = rep(NA_real_, iter)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we've got everything, so LET'S START THE ESTIMATION\n",
    "for(i in 1:iter){\n",
    "    sess$run(train)\n",
    "    r$loss[i] = sess$run(loss)\n",
    "    r$prob[i] = sess$run(t_p)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat('Final probability that the coin lands heads up is ', sess$run(t_p), '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we finished the estimation process, we can check how the loss value and the estimated probability was changing during the iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly() %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~loss,\n",
    "    data = r\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly() %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~prob,\n",
    "    data = r\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Example - MLE of normal distribution parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    In this section, we will do the maximum likelihood estimation of parameters for a continuous probability distribution. In such cases we need to use an alternative likelihood definition:<br/>\n",
    "    $\\mathcal{L}(\\theta \\mid x) = f (X=x; \\theta)$<br/>\n",
    "    where $f (X=x; \\theta)$ is the value of the probability density function (<i>pdf</i>)\n",
    "</p>\n",
    "<p>\n",
    "    Since we are working with the normal distribution, which have two parameters (mean and standard deviation), the total likelihood of all observations takes form:\n",
    "    $\\mathcal{L}(mean, sd \\mid x_1, x_2, ..., x_n) = \\prod_{i=1}^n f (X=x_i; mean, sd)$,\n",
    "    and<br/>\n",
    "    $log\\mathcal{L}(mean, sd \\mid x_1, x_2, ..., x_n) = \\sum_{i=1}^n log f (X=x_i; mean, sd)$\n",
    "</p>\n",
    "<p>\n",
    "    <b>QUESTION</b>: is there any reason why we should prefer one of these forms over another? (e.g. a sum of logarithms over a product)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages we will need\n",
    "library(tensorflow)\n",
    "library(tidyverse)\n",
    "library(plotly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with generating some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd_mean = 3   # the mean of our distribution\n",
    "nd_sd = 2     # the standard deviation of our distribution\n",
    "nd_data = rnorm(10000, nd_mean, nd_sd) # and the actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to build a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mean = tf$Variable(0.0) # a variable for the mean of the distribution, let's set it initially to 0\n",
    "t_sd = tf$Variable(1.0)   # a variable for the sd of the distribution, let's set it initially to 1\n",
    "\n",
    "dist = tf$distributions$Normal(loc = t_mean, scale = t_sd) # a distribution we will use, it takes variables storing mean and sd as parameters\n",
    "\n",
    "lprobs = dist$log_prob(nd_data) # log-likelihoods of parameters for each observation, i.e. logarithms of pdf values assuming current t_mean and t_sd \n",
    "negLL = -tf$reduce_sum(lprobs)  # sum of observations likelihoods. It's negative, because it is a value we will be minimising \n",
    "\n",
    "# of course, we also need to establish the optimisation process...\n",
    "optimizer = tf$train$AdamOptimizer(0.1)\n",
    "train = optimizer$minimize(negLL)\n",
    "\n",
    "# ... and create a session\n",
    "sess = tf$Session() \n",
    "sess$run(tf$global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 1000 # number of iterations to run\n",
    "\n",
    "# table for storing model parameter optimisation progress log\n",
    "r = tibble(\n",
    "    it = 1:iter,\n",
    "    negLL = rep(NA_real_, iter),\n",
    "    mean = rep(NA_real_, iter),\n",
    "    sd = rep(NA_real_, iter)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we've got everything, so let's start\n",
    "for(i in 1:iter){\n",
    "    sess$run(train)\n",
    "    r$negLL[i] = sess$run(negLL)\n",
    "    r$mean[i] = sess$run(t_mean)\n",
    "    r$sd[i] = sess$run(t_sd)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when our model parameters were fitted to the data, we can check how it went. Let's start with the likelihood maximisation (or rather minimisation of the negative log-likelihood):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly() %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~negLL,\n",
    "    data = r\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check how values of mean and standard deviation were changing during iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly(data = r) %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~mean,\n",
    "    name = 'mean'\n",
    "  ) %>%\n",
    "  add_lines(\n",
    "    x = ~it,\n",
    "    y = ~sd,\n",
    "    name = 'sd'\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Exercise - A bribe-taking mayor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The mayor of Los Data-flowos is a well-known bribe-taker. The local data science club decided to find out in what conditions he is eager to take a bribe. They discovered two main factors that influence it.\n",
    "</p>\n",
    "<p>\n",
    "    The first one is the amount of money offered, which have a logarhitmic influence on the mayor, which is proportional to:<br/>\n",
    "    $mc \\cdot log(ao + ml)$, where<br/>\n",
    "    <ul>\n",
    "        <li><b>mc</b> - money coefficient, currently unknown and needs to be estimated</li>\n",
    "        <li><b>ao</b> - amount of money offered by a briber</li>\n",
    "        <li><b>ml</b> - location parameter of the amount of money offered, currently unknown and needs to be estimated</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "    The second one is the number of policemen potential briber knows, which have influence on mayor's decision proportional to:<br/>\n",
    "    $pc \\cdot np^{pp}$, where<br/>\n",
    "    <ul>\n",
    "        <li><b>pc</b> - coefficient for the number of known policemen, currently unknown and needs to be estimated</li>\n",
    "        <li><b>np</b> - number of policemen known by the briber</li>\n",
    "        <li><b>pp</b> - exponent for the number of policemen, currently unknown and needs to be estimated</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "    Members of the club were also able to collect some observations. Each consists of this observations consists of three features:\n",
    "    <ul>\n",
    "        <li><b>money</b> - amount of money offered</li>\n",
    "        <li><b>policemen</b> - number of policemen potential briber knows</li>\n",
    "        <li><b>bribe_accepted</b> - binary flag informing if mayor accepted the bribe</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "    Your task is to use TensorFlow to estimate parameter values of a model predicting if the mayor will take a bribe:\n",
    "    <ul>\n",
    "        <li><b>mc</b> called <b>v_money_coef</b> below</li>\n",
    "        <li><b>ml</b> called <b>v_money_loc</b> below</li>\n",
    "        <li><b>pc</b> called <b>v_policemen_coef</b> below</li>\n",
    "        <li><b>pp</b> called <b>v_policemen_pow</b> below</li>\n",
    "        <li><b>intercept</b> called <b>v_intercept</b> below</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bribers = read_csv('data/bribers.csv') %>%\n",
    "    mutate(\n",
    "        policemen = as.double(policemen)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a peek\n",
    "head(bribers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with loading the data into the graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_money = NA           # TODO: create a constant with amounts of money offered\n",
    "t_policemen = NA       # TODO: create a constant with numbers of policemen known\n",
    "t_bribe_accepted = NA  # TODO: create a constant with flags whether bribes was accepted in given attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and declaring variables for parameters you want to estimate. Set the initial value of each variable to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_money_loc = tf$abs(NA) # TODO: create a variable for money_loc (ml). Initialise it with 1.0. (later we only use absolute value to avoid negatives)\n",
    "v_money_coef = NA        # TODO: create a variable for money_coef (mc). Initialise it with 1.0.\n",
    "\n",
    "v_policemen_pow = NA     # TODO: create a variable for policemen_power (pp). Initialise it with 1.0.\n",
    "v_policemen_coef = NA    # TODO: create a variable for policemen_coef (pc). Initialise it with 1.0.\n",
    "\n",
    "v_intercept = NA         # TODO: create a variable for intercept. Initialise it with 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to build a graph describing, how probability of taking a bribe can be calculated using constants and variables. You also need to define a total loss, and the optimisation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money_comp = NA     # TODO: calculate money component as money_coef + log(money + money_loc)\n",
    "policemen_comp = NA # TODO: calculate policemen component as policemen_coef * (policemen ^ policemen_power)\n",
    "\n",
    "logit = NA          # TODO: calculate logit as a sum of money component, policemen component and intercept\n",
    "prob = NA           # TODO: convert logit to a probability using a sigmoid function\n",
    "\n",
    "loss = NA           # TODO: calculate mean cross-entropy loss of the estimated probability\n",
    "\n",
    "optimizer = NA      # TODO: create an Adam optimiser with a learning rate equal 0.05\n",
    "train = NA          # TODO: use optimiser to minimise loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, it's time to create a session, and run the optimisation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf$Session()\n",
    "sess$run(tf$global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in 0:1000){\n",
    "    sess$run(train)\n",
    "    if(i %% 100 == 0){\n",
    "        cat(i, ') ', format(Sys.time(), \"%Y-%m-%d %X\"), ' Loss: ', sess$run(loss), '\\n', sep = '')\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When optimisation is finished, we can read parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(\"v_money_loc: \", sess$run(v_money_loc), \"\\n\")\n",
    "cat(\"v_money_coef: \", sess$run(v_money_coef), \"\\n\")\n",
    "cat(\"v_policemen_pow: \", sess$run(v_policemen_pow), \"\\n\")\n",
    "cat(\"v_policemen_coef: \", sess$run(v_policemen_coef), \"\\n\")\n",
    "cat(\"v_intercept: \", sess$run(v_intercept), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Local fortune-teller is sure that these parameters should be equal:<br/>\n",
    "    money_loc = 2<br/>\n",
    "    money_coef = 1<br/>\n",
    "    policemen_pow = 1.3<br/>\n",
    "    policemen_coef = -0.5<br/>\n",
    "    intercept = -1.5<br/>\n",
    "    It seems these numbers are correct, as they often worked. Are your estimates similar?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
