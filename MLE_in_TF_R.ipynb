{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find the documentation at:\n",
    "\n",
    "- [TenorFlow in R](https://tensorflow.rstudio.com) - The R package we're using\n",
    "- [TensorFlow Python API](https://www.tensorflow.org/api_docs/python/tf) - TensorFlow itself (the documentation is for a Python library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Computation Graphs and Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, let's load the packages we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Sessions represent a connection between the client program and the C++ runtime. We need them to run all computations, e.g. because they store values of variables. Let's create one:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf$Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we finish working with a graph, we should close the session, to free up the resources associated with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, since we will need a session, let's create the next one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf$Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Computation graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Computations in TensorFlow are expressed as Computation Graphs, which consist of multiple types of nodes:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - constants\n",
    "<p>Constants are tensors the values of which cannot be changed after their creation. We can use them to store observation data, esp. if we would like to estimate our model parameters using General Purpose Graphics Processing Units (GPGPUs). This way we will avoid continuous data transfer between GPGPU and the main computer memory.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnst = tf$constant(42) # let's define a constant which contains the answer to life, the universe and everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>sess$run(var)</b> means \"evaluate and return the value of a node which is indicated by the variable <b>var</b>\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(cnst) # and now let's read its value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Constants and other tensors do not need to be scalars. They can be vectors or matrices having 2 or more dimensions<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = 1:10\n",
    "cnst_vec = tf$constant(vec) # a constant containing a vector\n",
    "sess$run(cnst_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = matrix(1:16, 4, 4)\n",
    "cnst_mat = tf$constant(mat) # a constant containing a 2D matrix\n",
    "sess$run(cnst_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - variables\n",
    "<p>Variables are tensors the values of which can be changed when operations are run. They can be initialised using a constant or a random value from a selected distribution. We can use them to represent model parameters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrbl = tf$Variable(                                           # let's create a variable\n",
    "    tf$random_normal(shape = shape(1), mean = 0, stddev = 1)  # that will be a single scalar, initialised from normal(0,1) distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(tf$global_variables_initializer()) # let's initialise our variable (it needs to be run, even if we initialize a variable with a constant)\n",
    "# because, we initialise this variable using a random distribution, its value will change every time we will rerun this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(vrbl) # and check its value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - placeholders\n",
    "<p>Placeholders are tensors that we can use to pass values in the moment of running a graph. When we define a placeholder, we need to provide its values at the moment of running a node, which either is that placeholder, or which depends directly or indirectly on that placeholder. In general, they behave similarly to constants, but when we use them, their values need to be transferred to the memory of the processing unit that is running the graph. It's not a big deal if the processing is done on a local CPU – but it will slow down computations run on the GPU.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phld = tf$placeholder(tf$float32) # let's define a placeholder, which will expect to have a 32-bit float (not double) assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(phld) # when we run a graph that contains placeholders without setting their values, it throws an error, because the value of a placeholder isn't set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(phld, feed_dict = dict(phld = 8)) # when we set a value to a placeholder when running a graph that contains it, its value will be returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - operations\n",
    "<p>Operations are nodes that do the actual calculations. They take tensors as inputs and produce tensors as outputs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf$constant(1L) # let's define two constant nodes...\n",
    "b = tf$constant(2L)\n",
    "\n",
    "c = a + b # ... and a node which is equal to their sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_2_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(c) # yep, it's confirmed, 1 + 2 = 3. Math in TensorFlow works like anywhere else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Example - Pythagorean theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this example, we will create a graph that uses the Pythagorean theorem to calculate the length of the <i>hypotenuse</i> when having the lenghts of <i>legs</i> provided as inputs. The equation we need is:</p>\n",
    "<p>$c = \\sqrt{a^2 + b^2}$</p>\n",
    "<p>Let's express it as a computation graph:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf$placeholder(tf$float32) # placeholder for two sides lengths\n",
    "b = tf$placeholder(tf$float32)\n",
    "\n",
    "c_square = a^2 + b^2  # now we need to calculate sum of legs lengths squares (or rather create a node that computes it)\n",
    "\n",
    "c = tf$sqrt(c_square) # and now we can calculate the square root of c_square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The graph we created above looks like:</p>\n",
    "\n",
    "![](img/1_3_pyth_graph.png)\n",
    "\n",
    "<p>Now we can evaluate the <i>c</i> node value in a session:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(c, feed_dict = dict(a = 3, b = 4)) # because a and b are placeholders, and they're needed for c node calculation\n",
    "                                            # we need to assign a and b values when running the c node calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(c(a = a, b = b, c_2 = c_square, c = c), feed_dict = dict(a = 3, b = 4)) # of course, we can also run multiple nodes in one call.\n",
    "                                                                                 # then value of each node is returned in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Example - vector cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we will create a graph that will calculate the value of a cosine between two vectors. The equation we will use is:</p>\n",
    "$\\cos(∡\\mathbf{A}\\mathbf{B}) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i  B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}  \\sqrt{\\sum\\limits_{i=1}^{n}{B_i^2}} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_a = tf$placeholder(tf$float64)\n",
    "vec_b = tf$placeholder(tf$float64)\n",
    "\n",
    "dotprod_ab = tf$tensordot(vec_a, vec_b, 1L) # 1L as we need to pass an integer 1\n",
    "len_a = tf$norm(vec_a)\n",
    "len_b = tf$norm(vec_b)\n",
    "\n",
    "len_ab_prod = len_a * len_b\n",
    "\n",
    "cos_ab = dotprod_ab / len_ab_prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_4_cosine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(cos_ab, feed_dict = dict(vec_a = c(3,4,5,6), vec_b = c(6,8,10,12))) # the result here should be 1, because\n",
    "                                                                             # these vectors have exactly the same direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(cos_ab, feed_dict = dict(vec_a = c(3,4), vec_b = c(-4,3))) # the result here should be 0, because these vectors are perpendicular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(cos_ab, feed_dict = dict(vec_a = c(3,4,10), vec_b = c(-3,-4,-10))) # the result here should be -1, because a points in the opposite direction than b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Build a graph which will calculate the value of covariance between <i>x</i> and <i>y</i> vectors which should be provided as placeholders' values during a session$run() call.</p>\n",
    "\n",
    "$cov(x,y) = {\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y}) \\over n-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = NA # TODO: define a placeholder for x\n",
    "y = NA # TODO: define a placeholder for y\n",
    "\n",
    "mean_x = NA # TODO: calculate the mean of x, you shoud use tf$reduce_mean()\n",
    "mean_y = NA # TODO: calculate the mean of y, you shoud use tf$reduce_mean()\n",
    "\n",
    "n_1 = tf$cast(tf$size(x), tf$float64) - 1 # number of elements in x minus 1\n",
    "\n",
    "diff_x = NA # TODO: calculate the difference between vector x and its mean\n",
    "diff_y = NA # TODO: calculate the difference between vector y and its mean\n",
    "\n",
    "diff_xy_prod = NA # TODO: calculate the dot product of diff_x and diff_y. You can use tf$reduce_sum or tf$tensordot\n",
    "\n",
    "cov_xy = NA # TODO: divide diff_xy_prod by n - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess$run(cov_xy, feed_dict = dict(x = c(5,20,40), y = c(10,24,33)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The expected result is: <i>199.166666666667</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Loss function is used to measure how well a given model performs in terms of predicting the expected value. Various loss functions compare predicted and expected values differently. The most popular loss functions are MSE, MAE and cross-entropy. \n",
    "    <br>\n",
    "    <b>MSE</b> or <b>MAE</b> are used when a specific value is estimated (e.g. height in cm). \n",
    "<br>\n",
    "    <b>Cross-entropy</b> is used when a probability is estimated (e.g. probability that a student will answer a question correctly).   \n",
    "</p>\n",
    "<p><i>Usually name 'loss function' is used for a single observation and 'cost function' for an entire dataset. However, in tensorflow environment name 'loss function' is used for entire dataset as well. </i>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Example - MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> MSE - Mean Squared Error </b> is a loss function represented by the following equation:</p> \n",
    "$MSE = {1 \\over{n}} \\sum\\limits_{i=1}^{n}{(\\overline{y_i} - y_i)^2}$\n",
    "<br>\n",
    "where <br>\n",
    "$n$ - number of observations<br>\n",
    "$\\overline{y_i}$ - value predicted by the model for observation $i$ <br>\n",
    "$y_i$ - actual value for observation $i$ <br>\n",
    "\n",
    "<p> In this example, we will:\n",
    "    <ul>\n",
    "        <li> implement MSE from scratch in plain R, </li>\n",
    "        <li> use already implemented MSE in tensorflow.</li>\n",
    "    </ul>\n",
    "</p>     \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data\n",
    "set.seed(1234)\n",
    "y <- 1:10 \n",
    "(y_ <- y + rnorm(10, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE from scratch in R\n",
    "MSE <- mean((y_-y)^2)\n",
    "sprintf('MSE calculated from scratch in R: %.5f', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressing data as nodes\n",
    "ty <- tf$constant(y, dtype = 'float32') # constant - it is real value\n",
    "ty_ <- tf$constant(y_, dtype = 'float32') # now it's a constant as we only do math operations; \n",
    "                                          # however, usually it's Variable (changes over time during training)\n",
    "\n",
    "# MSE already implemented in tensorflow\n",
    "MSE_tf <- tf$losses$mean_squared_error(labels = ty, predictions = ty_)\n",
    "sprintf('MSE already implemented in tensorflow: %.5f', sess$run(MSE_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Exercise - MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> MAE - Mean Absolute Error </b> is a loss function represented by the following equation:</p> \n",
    "$MAE = {1 \\over{n}} \\sum\\limits_{i=1}^{n}{\\mid \\overline{y_i} - y_i \\mid}$\n",
    "<br>\n",
    "where <br>\n",
    "$n$ - number of observations<br>\n",
    "$\\overline{y_i}$ - value predicted by the model for observation $i$ <br>\n",
    "$y_i$ - actual value for observation $i$ <br>\n",
    "<br>\n",
    "<p> In this exercise, try to:\n",
    "    <ul>\n",
    "        <li> implement MAE from scratch in plain R, </li>\n",
    "        <li> use already implemented MAE in tensorflow</li>\n",
    "    </ul>\n",
    "    as it was done in the example above.\n",
    "</p>     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data\n",
    "set.seed(1234)\n",
    "y <- 1:10 \n",
    "(y_ <- y + rnorm(10, 0, 1))\n",
    "\n",
    "# Expressing data as nodes\n",
    "ty <- tf$constant(y, dtype = 'float32') # constant - it is real value\n",
    "ty_ <- tf$constant(y_, dtype = 'float32') # now it's a constant as we only do math operations; \n",
    "                                          # however, usually it's Variable (changes over time during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE <- NA # TODO: calculate MAE in plain R\n",
    "sprintf('MSE calculated from scratch in R: %.5f', MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_tf <- NA # TODO: find appropriate function already implemented in tensorflow \n",
    "sprintf('MAE already implemented in tensorflow: %.5f', sess$run(MAE_tf)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected value for each run is: $0.84257$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Example - cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> Cross-entropy </b> is a loss function represented by the following equation:</p> \n",
    "$cross-entropy = -{1 \\over{n}} \\sum\\limits_{i=1}^{n} {y_i \\cdot log \\overline{y_i} + (1 - y_i) \\cdot log (1 - \\overline{y_i})}$\n",
    "<br>\n",
    "where <br>\n",
    "$n$ - number of observations<br>\n",
    "$\\overline{y_i}$ - probability predicted by the model for observation $i$ <br>\n",
    "$y_i$ - actual outcome for observation $i$ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty <- tf$constant(rep(c(1,0), 5))\n",
    "ty_ <- tf$random_uniform(shape(10), minval = 0, maxval = 1, seed = 12345)\n",
    "\n",
    "sess$run(ty)\n",
    "sess$run(ty_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce <- tf$losses$log_loss(labels = ty, predictions = ty_)\n",
    "sess$run(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The goal of training the model is to find the best fitting parameters. Loss function is used to evaluate model performance. To find the best estimators, we need to minimize this loss. Gradient descent algorithm is known as the first-order iterative optimizer. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent rule\n",
    "\n",
    "Updating parameters $\\vec{\\theta} = [{\\theta}_0, {\\theta}_1, ..., {\\theta}_k] $ in iterations:\n",
    "<br><br>\n",
    "${\\theta}_i := \\theta_i - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_i} loss(\\vec{\\theta}, X)$\n",
    "\n",
    "where <br>\n",
    "${\\theta}_i$ - $i$ parameter <br>\n",
    "$\\alpha$ - learning rate (hyperparameter) <br>\n",
    "$\\frac{\\partial}{\\partial \\theta_i} loss(\\vec{\\theta}, X)$ - partial derivative of loss function for parameter ${\\theta}_i$<br>\n",
    "$X$ - observations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "![](img/2_2_Gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Example - one number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We can implement gradient descent from scratch in plain R. To do it, we need to calculate the derivative of our loss function.\n",
    "Let's assume that:\n",
    "    <ul>\n",
    "        <li>we use MSE loss function,</li>\n",
    "        <li>we have only one training example ($n = 1$),</li>\n",
    "        <li>$\\overline{y} = \\theta_0$.</li> \n",
    "       </ul>\n",
    "Then $MSE = {1 \\over{n}} \\sum\\limits_{i=1}^{n}{(\\overline{y_i} - y_i)^2}$ for $n = 1$ is:\n",
    "    <br>\n",
    "    $ MSE = ( \\overline{y_1} - y_1)^2$ \n",
    "    <br>\n",
    "    and \n",
    "    <br>\n",
    "    $ \\frac{\\partial}{\\partial \\theta_0 } MSE(\\theta_0) = 2 \\cdot (\\overline {y_1} - y_1)$\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y <- 10 # assigning real value \n",
    "y_ <- 15 # assigning initial value of estimated parameter\n",
    "\n",
    "for (i in 1:10){\n",
    "  dy_ <- 2*(y_-y) # calculating derivative of loss function \n",
    "  y_ <- y_ - 0.1 * dy_ # updating estimated value according to gradient descent with learning rate = 0.1\n",
    "  print(y_)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent optimizer is already implemented in tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty <- tf$constant(10, dtype = 'float32') # real value \n",
    "ty_ <- tf$Variable(15, dtype = 'float32') # variable which we estimate; generally it's an operation node \n",
    "                                          # with result that depends on some variables, and the optimization process\n",
    "                                          # optimizes values of these variables\n",
    "sess$run(tf$variables_initializer(list(ty_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer <- tf$train$GradientDescentOptimizer(learning_rate = 0.1) # defining optimizer with learning rate = 0.1\n",
    "loss <- tf$losses$mean_squared_error(labels = ty, predictions = ty_) # defining loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i in 1:10){\n",
    "  train <- optimizer$minimize(loss) # updating values with gradient descent optimizer which minimize MSE loss function\n",
    "  sess$run(train)\n",
    "  print(sess$run(ty_))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Example - linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let us use more complex example to show how gradient descent works. Our task is to estimate $a, b$ and $c$ parameters in a linear model: \n",
    "<br>\n",
    "$z = a \\cdot x + b \\cdot y + c $  \n",
    "using gradient descent to minimize MSE loss function. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_222 <- readr::read_csv('data/example_222.csv') # read data\n",
    "head(example_222) # look at the data \n",
    "\n",
    "tx <- tf$constant(example_222$x, dtype = 'float32') # create node \n",
    "ty <- tf$constant(example_222$y, dtype = 'float32') # create node\n",
    "tz <- tf$constant(example_222$z, dtype = 'float32') # create node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta <- tf$Variable(2) # init parameters\n",
    "tb <- tf$Variable(4) # init parameters\n",
    "tc <- tf$Variable(-14) # init parameters\n",
    "sess$run(tf$global_variables_initializer())\n",
    "\n",
    "tz_ <- ta*tx + tb*ty + tc # create node to compute estimation of z \n",
    "\n",
    "# MODEL \n",
    "optimizer <- tf$train$GradientDescentOptimizer(learning_rate = .005) # choose Gradient Descent as optimizer\n",
    "MSE <- tf$losses$mean_squared_error(labels = tz, predictions = tz_) # choose MSE as loss function\n",
    "train <- optimizer$minimize(MSE) # minimizing MSE with Gradient Descent optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter <- 100 # number of iterations in training \n",
    "\n",
    "# helper table to save results\n",
    "results <- tibble::tibble(\n",
    "  it = 0:iter,  \n",
    "  mse = c(sess$run(MSE), rep(NA_real_, iter)),\n",
    "  a = c(sess$run(ta), rep(NA_real_, iter)),\n",
    "  b = c(sess$run(tb), rep(NA_real_, iter)),\n",
    "  c = c(sess$run(tc), rep(NA_real_, iter)))\n",
    "\n",
    "results[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING \n",
    "for (i in 1:iter){\n",
    "\n",
    "  sess$run(train) # one training\n",
    "  \n",
    "  # save results\n",
    "  results$it[i+1] <- i\n",
    "  results$mse[i+1] <- sess$run(MSE)\n",
    "  results$a[i+1] <- sess$run(ta) \n",
    "  results$b[i+1] <- sess$run(tb)\n",
    "  results$c[i+1] <- sess$run(tc)    \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check results\n",
    "head(results)\n",
    "tail(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(magrittr)\n",
    "library(plotly)\n",
    "\n",
    "# Let's plot how loss has changed during training\n",
    "results %>% \n",
    "  plot_ly(x=~it, y=~mse) %>% \n",
    "  add_lines() %>% \n",
    "  layout(xaxis = list(title = 'Iteration'), \n",
    "         yaxis = list(title = 'Loss (mse)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot how parameters have changed during training\n",
    "results %>% \n",
    "  tidyr::gather(key, Values, -c(it, mse)) %>% \n",
    "  plot_ly(x = ~it, y = ~ Values, color = ~key) %>% \n",
    "  add_lines() %>% \n",
    "  layout(xaxis = list(title = 'Iteration')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Exercise - lasso regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> Linear regression model </b> is a model which tries to fit the best straight line to the data by minimizing MSE. \n",
    "    <br>\n",
    "    <b> Lasso </b> and <b> ridge linear regression models </b> are models which also try to fit the best straight line to the data, but by <b>minimizing MSE plus some additional penalty component</b>.\n",
    "    <br> \n",
    "    <br>\n",
    "    For linear model: $z = a \\cdot x + b \\cdot y + c $, <br>\n",
    "    where $a, b, c$ are parameters  \n",
    "    <ul>\n",
    "        <li>ridge regression minimizes: $MSE + \\lambda \\cdot (a^2 + b^2)$ </li>\n",
    "        <li>lasso regression minimizes: $MSE + \\lambda \\cdot (\\mid a \\mid  + \\mid b \\mid)$ </li>\n",
    "    </ul> \n",
    "    where $\\lambda$ is a hyperparameter which termines how severe the additional penalty is.\n",
    "</p>\n",
    "<br>\n",
    "    More details about ridge and lasso regressions in videos:\n",
    "\n",
    "- [Ridge Regression](https://www.youtube.com/watch?v=Q81RR3yKn30)\n",
    "- [Lasso Regression](https://www.youtube.com/watch?v=NGf0voTMlcs) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when you know how loss functions for ridge and lasso regression models look like, try to implement <b>lasso regression </b>on data from the example above (2.2.2 Example - linear regression).\n",
    "<br> \n",
    "<br>\n",
    "The task is to estimate $a, b$ and $c$ parameters in $z = a \\cdot x + b \\cdot y + c$   model using lasso regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data and creating nodes\n",
    "\n",
    "example_222 <- readr::read_csv('data/example_222.csv') # read data\n",
    "head(example_222) # look at the data \n",
    "tx <- tf$constant(example_222$x, dtype = 'float32') # create node \n",
    "ty <- tf$constant(example_222$y, dtype = 'float32') # create node\n",
    "tz <- tf$constant(example_222$z, dtype = 'float32') # create node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta <- NA # TODO: init parameter a=2\n",
    "tb <- NA # TODO: init parameter b=4\n",
    "tc <- NA # TODO: init parameter c=-14\n",
    "\n",
    "sess$run(tf$global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_ <- NA # TODO: create node for z (node with equation which represents linear model)\n",
    "\n",
    "optimizer <- NA # TODO: select gradient descent optimizer and set learning rate=0.005\n",
    "mse <- NA # TODO: select mse loss function\n",
    "loss <- NA # TODO: create whole loss function: to mse add additional component for lasso regression and lambda = 0.2\n",
    "train <- NA # TODO: minimize loss function\n",
    "\n",
    "model <- list(mse = mse, \n",
    "             loss = loss, \n",
    "             optimizer = optimizer, \n",
    "             train = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter <- 100\n",
    "\n",
    "results <- tibble::tibble(\n",
    "  it = 0:iter,  \n",
    "  mse = c(sess$run(mse), rep(NA_real_, iter)),\n",
    "  loss = c(sess$run(loss), rep(NA_real_, iter)),  \n",
    "  a = c(sess$run(ta), rep(NA_real_, iter)),\n",
    "  b = c(sess$run(tb), rep(NA_real_, iter)),\n",
    "  c = c(sess$run(tc), rep(NA_real_, iter)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i in 1:iter){\n",
    "\n",
    "  NA # TODO: run a single training iteration\n",
    "  \n",
    " # save results\n",
    "  results$it[i+1] <- i\n",
    "  results$mse[i+1] <- sess$run(mse)\n",
    "  results$loss[i+1] <- sess$run(loss)\n",
    "  results$a[i+1] <- sess$run(ta) \n",
    "  results$b[i+1] <- sess$run(tb)\n",
    "  results$c[i+1] <- sess$run(tc)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(results)\n",
    "tail(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Other optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent algorithm is an example of optimizers. However, there are also others gradient descent based optimizers:\n",
    "<ul> \n",
    "    <li><b>[Momentum optimizer](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer)</b> - helps accelerate gradient descent in the right direction and decrease oscillations by keeping the momentum.\n",
    "    <li><b>[RMSProp optimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer)</b> - has the same aim as momentum optimizer, however there is a difference in formula.\n",
    "    </li>\n",
    "    <li><b>[Adam optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)</b> - mixes Momentum and RMSProp approaches</li>\n",
    "    <li> ...</li>\n",
    "    </ul>\n",
    "Let's see how to call different optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expressing data in nodes\n",
    "ty <- tf$constant(10, dtype = 'float32') # real value \n",
    "ty_ <- tf$Variable(15, dtype = 'float32') # variable which we estimate; generally it's an operation node \n",
    "\n",
    "loss <- tf$losses$mean_squared_error(labels = ty, predictions = ty_) # loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer <- tf$train$GradientDescentOptimizer(learning_rate = 0.005) # gradient descent optimizer\n",
    "train <- optimizer$minimize(loss) # train \n",
    "\n",
    "sess$run(tf$global_variables_initializer()) # init variables\n",
    "\n",
    "# Gradient descent\n",
    "for (i in 1:10)\n",
    "{sess$run(train)\n",
    "  print(sess$run(ty_))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer <- tf$train$MomentumOptimizer(learning_rate = 0.005, momentum = 0.8) # momentum optimizer\n",
    "train <- optimizer$minimize(loss) # train \n",
    "\n",
    "sess$run(tf$global_variables_initializer()) # init variables\n",
    "\n",
    "# Momentum optimizer\n",
    "for (i in 1:10)\n",
    "{sess$run(train)\n",
    "  print(sess$run(ty_))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Pythagorean theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We know the values of $b=12$ and $c=13$ and we know that $a,b$ and $c$ satisfy the Pythagorean theorem:  $c = \\sqrt{a^2 + b^2}$.\n",
    "Let's assume that we are not able to transform this equation to calculate $a$ value. \n",
    "    <br> The task is to estimate $a$ when we know $b$ and $c$ values and Pythagorean theorem.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_b <- NA # TODO: create node with b = 12\n",
    "tf_c <- NA # TODO: create node with c = 13\n",
    "tf_a <- NA # TODO: create node for a; initialize it with any value you want\n",
    "\n",
    "c_ <- NA # TODO: create node with c_ which follows the Pythagorean theorem\n",
    "\n",
    "loss <- NA # TODO: as loss function choose MSE\n",
    "optimizer <- NA # TODO: as optimizer choose Adam optimizer, choose the learning rate \n",
    "train <- NA # TODO: set optimizer to minimize loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA # TODO: initialize all variables\n",
    "\n",
    "print(paste0('current loss: ', sess$run(loss))) # print current value of loss function for randomly initialized value of a\n",
    "\n",
    "NA # TODO: do one iteration of training\n",
    "\n",
    "print(paste0('loss after one iteration of training: ', sess$run(loss))) # print current value of loss function after one iteration\n",
    "print(paste0('value of a after one iteration of training: ', sess$run(tf_a))) # print current value of a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results <- tibble::tibble(current_loss = sess$run(loss), \n",
    "                   current_value = sess$run(tf_a))\n",
    "                   \n",
    "for (i in 1:200){\n",
    "\n",
    "  NA # TODO:  do one iteration of training\n",
    "\n",
    "  # save results to training table\n",
    "  results <- results %>% \n",
    "    rbind(tibble::tibble(current_loss = sess$run(loss), \n",
    "                 current_value = sess$run(tf_a)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot how loss and parameter value have changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results %>% \n",
    "  plot_ly(x = 1:nrow(.), y = ~current_loss) %>% \n",
    "  add_lines() %>% \n",
    "  layout(xaxis = list(title = 'Iteration'), \n",
    "         yaxis = list(title = 'Loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results %>% \n",
    "  plot_ly(x = 1:nrow(.), y = ~current_value) %>% \n",
    "  add_lines() %>% \n",
    "  layout(xaxis = list(title = 'Iteration'), \n",
    "         yaxis = list(title = 'Value of a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Times table (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 different numbers. We don't know their values. However, we know the products of all combinations of those numbers (100 products). The goal is to estimate values of those 10 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_table <- readr::read_csv('data/example_times_table.csv') %>% # read data\n",
    "    mutate(\n",
    "        a_id = as.integer(a_id),\n",
    "        b_id = as.integer(b_id)\n",
    "    )\n",
    "head(times_table) # look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read data: \n",
    "- The first number squared equals 4\n",
    "- Product of the second and the first numbers equals 6\n",
    "- Product of the third and the first numbers equals 10 \n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_indices <- NA # TODO: create node for a indices (from times_table) with dtype = 'int32'\n",
    "b_indices <- NA # TODO: create node for b indices (from times_table) with dtype = 'int32'\n",
    "y <- NA # TODO: create node for a product (from times_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values <- NA # TODO: create node for values of parameters which we'll estimate,\n",
    "            # we'll estimate 10 numbers (length(unique(times_table$a_id)))\n",
    "            # init values from uniform distribution [1,10]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to match indices with their values (that is: each factor ($a$ and $b$) with index 0 is matched with the first value from tensor <b>values</b>, each factor ($a$ and $b$) with index 1 is matched with the second value from tensor <b>values</b> and so on). To do it, we use function <b>gather</b>. [Here more information about gather.](https://www.tensorflow.org/api_docs/python/tf/gather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_a_gathered <- tf$gather(values, a_indices) # gather a parameters with indices\n",
    "t_b_gathered <- tf$gather(values, b_indices) # gather b parameters with indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ <- NA # TODO: create node which is a product of a and b; NOTE: you need to use gathered values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL \n",
    "loss <- NA # TODO: as loss function choose MSE\n",
    "optimizer <- NA # TODO: as optimizer choose Adam optimizer, choose the learning rate \n",
    "train <- NA # TODO: set optimizer to minimize loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA # TODO: init all variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tibble)\n",
    "# let us take a look at the initialized values of t_a_gathered, t_b_gathered and y_\n",
    "print('a values:')\n",
    "sess$run(t_a_gathered)\n",
    "print('b values:')\n",
    "sess$run(t_b_gathered)\n",
    "print('products for iteration 0:')\n",
    "sess$run(y_)\n",
    "\n",
    "# Let us check what is a loss for randomly initialized values\n",
    "print(paste0('loss for iteration 0: ', sess$run(loss)))\n",
    "\n",
    "# Let us prepare a helper table losses with step number and loss value\n",
    "step = 0\n",
    "losses = tibble(\n",
    "    step = step,\n",
    "    current_loss = sess$run(loss)\n",
    ")\n",
    "\n",
    "# Let us prepare a helper table params with step number, values of parameters for each index in a given estimation step, \n",
    "# and indices 0:9 for each value (just to plot it later)\n",
    "params <- tibble(step = step,\n",
    "                 values_calc = sess$run(values),\n",
    "                 id = 0:9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN \n",
    "\n",
    "for (i in 1:500) {\n",
    "    \n",
    "    NA # TODO: one iteration of training\n",
    "    \n",
    "    losses = rbind(\n",
    "        losses,\n",
    "        tibble(step = step + i,\n",
    "               current_loss = sess$run(loss))\n",
    "    )\n",
    "    \n",
    "    params = rbind(\n",
    "        params, \n",
    "        tibble(step = step,\n",
    "                 values_calc = sess$run(values),\n",
    "                 id = 0:9)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check our results\n",
    "tail(losses, 5)\n",
    "tail(params, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot loss function\n",
    "\n",
    "losses %>% \n",
    "  plot_ly(x = ~step, y = ~current_loss) %>% \n",
    "  add_lines() %>% \n",
    "  layout(xaxis = list(title = 'Iteration'), \n",
    "         yaxis = list(title = 'Loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
